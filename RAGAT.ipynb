{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9vTDPByuT7L"
      },
      "source": [
        "# RAGAT: Relation Aware Graph Attention Network for Knowledge Graph Completion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgLWnq48uJtU"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://github.com/liuxiyang641/RAGAT/raw/main/model.png\">\n",
        "</center>\n",
        "\n",
        "**Source:** [https://github.com/liuxiyang641/RAGAT](https://github.com/liuxiyang641/RAGAT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHYutNYA6he1"
      },
      "source": [
        "## Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhCSr_ye6ECo",
        "outputId": "0b7c5934-32fb-430b-cac7-7970017fc741"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nI9BCIyhzSZL",
        "outputId": "206fa300-e9cc-46c3-afec-d4424fb21964"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ordered-set\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: ordered-set\n",
            "Successfully installed ordered-set-4.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install ordered-set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URMIA6Fw5fTW"
      },
      "outputs": [],
      "source": [
        "import traceback, sys, os, random, pdb, json, uuid, time, argparse, inspect\n",
        "\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "import logging, logging.config\n",
        "from collections import defaultdict as ddict\n",
        "from ordered_set import OrderedSet\n",
        "\n",
        "# PyTorch related imports\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.init import xavier_normal_\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn import Parameter\n",
        "from torch.utils.data import Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2Idd_MwW7_b"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmhZpgavW9LY",
        "outputId": "cfa017a9-1393-4c6f-f354-6f7e2284baae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'RAGAT'...\n",
            "remote: Enumerating objects: 63, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 63 (delta 17), reused 17 (delta 17), pack-reused 38\u001b[K\n",
            "Unpacking objects: 100% (63/63), 6.85 MiB | 4.93 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/liuxiyang641/RAGAT.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_weNtGeXV4c"
      },
      "outputs": [],
      "source": [
        "!mv RAGAT/data ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swLvhNiWBn10"
      },
      "outputs": [],
      "source": [
        "!mv RAGAT/config ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqeX0chmXklg"
      },
      "outputs": [],
      "source": [
        "!rm -r RAGAT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpPOr3rxWYI4"
      },
      "source": [
        "## Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AJwl89aWZ41"
      },
      "outputs": [],
      "source": [
        "class TrainDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Training Dataset class.\n",
        "    Parameters\n",
        "    ----------\n",
        "    triples:\tThe triples used for training the model\n",
        "    params:\t\tParameters for the experiments\n",
        "    Returns\n",
        "    -------\n",
        "    A training Dataset class instance used by DataLoader\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, triples, params):\n",
        "        self.triples = triples\n",
        "        self.p = params\n",
        "        self.entities = np.arange(self.p.num_ent, dtype=np.int32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.triples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ele = self.triples[idx]\n",
        "        triple, label, sub_samp = torch.LongTensor(ele['triple']), np.int32(ele['label']), np.float32(ele['sub_samp'])\n",
        "        trp_label = self.get_label(label)\n",
        "\n",
        "        if self.p.lbl_smooth != 0.0:\n",
        "            trp_label = (1.0 - self.p.lbl_smooth) * trp_label + (1.0 / self.p.num_ent)\n",
        "\n",
        "        if self.p.strategy == 'one_to_n':\n",
        "            return triple, trp_label, None, None\n",
        "\n",
        "        elif self.p.strategy == 'one_to_x':\n",
        "            sub_samp = torch.FloatTensor([sub_samp])\n",
        "            neg_ent = torch.LongTensor(self.get_neg_ent(triple, label))\n",
        "            return triple, trp_label, neg_ent, sub_samp\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        # return triple, trp_label, None, None\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_fn(data):\n",
        "        triple = torch.stack([_[0] for _ in data], dim=0)\n",
        "        trp_label = torch.stack([_[1] for _ in data], dim=0)\n",
        "        # triple: (batch-size) * 3(sub, rel, -1) trp_label (batch-size) * num entity\n",
        "        # return triple, trp_label\n",
        "        if not data[0][2] is None:  # one_to_x\n",
        "            neg_ent = torch.stack([_[2] for _ in data], dim=0)\n",
        "            sub_samp = torch.cat([_[3] for _ in data], dim=0)\n",
        "            return triple, trp_label, neg_ent, sub_samp\n",
        "        else:\n",
        "            return triple, trp_label\n",
        "\n",
        "    # def get_neg_ent(self, triple, label):\n",
        "    #     def get(triple, label):\n",
        "    #         pos_obj = label\n",
        "    #         mask = np.ones([self.p.num_ent], dtype=np.bool)\n",
        "    #         mask[label] = 0\n",
        "    #         neg_ent = np.int32(\n",
        "    #             np.random.choice(self.entities[mask], self.p.neg_num - len(label), replace=False)).reshape([-1])\n",
        "    #         neg_ent = np.concatenate((pos_obj.reshape([-1]), neg_ent))\n",
        "    #\n",
        "    #         return neg_ent\n",
        "    #\n",
        "    #     neg_ent = get(triple, label)\n",
        "    #     return neg_ent\n",
        "    def get_neg_ent(self, triple, label):\n",
        "        def get(triple, label):\n",
        "            if self.p.strategy == 'one_to_x':\n",
        "                pos_obj = triple[2]\n",
        "                mask = np.ones([self.p.num_ent], dtype=np.bool)\n",
        "                mask[label] = 0\n",
        "                neg_ent = np.int32(np.random.choice(self.entities[mask], self.p.neg_num, replace=False)).reshape([-1])\n",
        "                neg_ent = np.concatenate((pos_obj.reshape([-1]), neg_ent))\n",
        "            else:\n",
        "                pos_obj = label\n",
        "                mask = np.ones([self.p.num_ent], dtype=np.bool)\n",
        "                mask[label] = 0\n",
        "                neg_ent = np.int32(\n",
        "                    np.random.choice(self.entities[mask], self.p.neg_num - len(label), replace=False)).reshape([-1])\n",
        "                neg_ent = np.concatenate((pos_obj.reshape([-1]), neg_ent))\n",
        "\n",
        "                if len(neg_ent) > self.p.neg_num:\n",
        "                    import pdb;\n",
        "                    pdb.set_trace()\n",
        "\n",
        "            return neg_ent\n",
        "\n",
        "        neg_ent = get(triple, label)\n",
        "        return neg_ent\n",
        "\n",
        "    def get_label(self, label):\n",
        "        # y = np.zeros([self.p.num_ent], dtype=np.float32)\n",
        "        # for e2 in label: y[e2] = 1.0\n",
        "        # return torch.FloatTensor(y)\n",
        "        if self.p.strategy == 'one_to_n':\n",
        "            y = np.zeros([self.p.num_ent], dtype=np.float32)\n",
        "            for e2 in label: y[e2] = 1.0\n",
        "        elif self.p.strategy == 'one_to_x':\n",
        "            y = [1] + [0] * self.p.neg_num\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        return torch.FloatTensor(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ymy2XLAWgbR"
      },
      "outputs": [],
      "source": [
        "class TestDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Evaluation Dataset class.\n",
        "    Parameters\n",
        "    ----------\n",
        "    triples:\tThe triples used for evaluating the model\n",
        "    params:\t\tParameters for the experiments\n",
        "    Returns\n",
        "    -------\n",
        "    An evaluation Dataset class instance used by DataLoader for model evaluation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, triples, params):\n",
        "        self.triples = triples\n",
        "        self.p = params\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.triples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ele = self.triples[idx]\n",
        "        triple, label = torch.LongTensor(ele['triple']), np.int32(ele['label'])\n",
        "        label = self.get_label(label)\n",
        "\n",
        "        return triple, label\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_fn(data):\n",
        "        triple = torch.stack([_[0] for _ in data], dim=0)\n",
        "        label = torch.stack([_[1] for _ in data], dim=0)\n",
        "        return triple, label\n",
        "\n",
        "    def get_label(self, label):\n",
        "        y = np.zeros([self.p.num_ent], dtype=np.float32)\n",
        "        for e2 in label: y[e2] = 1.0\n",
        "        return torch.FloatTensor(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnKkj0H56gB4"
      },
      "source": [
        "## Helper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYQ5CBW66kB4"
      },
      "outputs": [],
      "source": [
        "def set_gpu(gpus):\n",
        "    \"\"\"\n",
        "    Sets the GPU to be used for the run\n",
        "    Parameters\n",
        "    ----------\n",
        "    gpus:           List of GPUs to be used for the run\n",
        "    Returns\n",
        "    -------\n",
        "    \"\"\"\n",
        "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sG2H9ErY67dN"
      },
      "outputs": [],
      "source": [
        "def get_logger(name, log_dir, config_dir):\n",
        "    \"\"\"\n",
        "    Creates a logger object\n",
        "    Parameters\n",
        "    ----------\n",
        "    name:           Name of the logger file\n",
        "    log_dir:        Directory where logger file needs to be stored\n",
        "    config_dir:     Directory from where log_config.json needs to be read\n",
        "    Returns\n",
        "    -------\n",
        "    A logger object which writes to both file and stdout\n",
        "    \"\"\"\n",
        "    config_dict = json.load(open(config_dir + 'log_config.json'))\n",
        "    config_dict['handlers']['file_handler']['filename'] = log_dir + name.replace('/', '-')\n",
        "    logging.config.dictConfig(config_dict)\n",
        "    logger = logging.getLogger(name)\n",
        "\n",
        "    std_out_format = '%(asctime)s - [%(levelname)s] - %(message)s'\n",
        "    consoleHandler = logging.StreamHandler(sys.stdout)\n",
        "    consoleHandler.setFormatter(logging.Formatter(std_out_format))\n",
        "    logger.addHandler(consoleHandler)\n",
        "\n",
        "    return logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1F9w6L416-65"
      },
      "outputs": [],
      "source": [
        "def get_combined_results(left_results, right_results):\n",
        "    results = {}\n",
        "    count = float(left_results['count'])\n",
        "\n",
        "    results['left_mr'] = round(left_results['mr'] / count, 5)\n",
        "    results['left_mrr'] = round(left_results['mrr'] / count, 5)\n",
        "    results['right_mr'] = round(right_results['mr'] / count, 5)\n",
        "    results['right_mrr'] = round(right_results['mrr'] / count, 5)\n",
        "    results['mr'] = round((left_results['mr'] + right_results['mr']) / (2 * count), 5)\n",
        "    results['mrr'] = round((left_results['mrr'] + right_results['mrr']) / (2 * count), 5)\n",
        "\n",
        "    for k in range(10):\n",
        "        results['left_hits@{}'.format(k + 1)] = round(left_results['hits@{}'.format(k + 1)] / count, 5)\n",
        "        results['right_hits@{}'.format(k + 1)] = round(right_results['hits@{}'.format(k + 1)] / count, 5)\n",
        "        results['hits@{}'.format(k + 1)] = round(\n",
        "            (left_results['hits@{}'.format(k + 1)] + right_results['hits@{}'.format(k + 1)]) / (2 * count), 5)\n",
        "    \n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTkgcPvv7Ar2"
      },
      "outputs": [],
      "source": [
        "def get_param(shape):\n",
        "    param = Parameter(torch.Tensor(*shape));\n",
        "    xavier_normal_(param.data)\n",
        "    return param"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYII-Atn7DUv"
      },
      "outputs": [],
      "source": [
        "def com_mult(a, b):\n",
        "    r1, i1 = a[..., 0], a[..., 1]\n",
        "    r2, i2 = b[..., 0], b[..., 1]\n",
        "    return torch.stack([r1 * r2 - i1 * i2, r1 * i2 + i1 * r2], dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bH4wzAEr7FXF"
      },
      "outputs": [],
      "source": [
        "def conj(a):\n",
        "    a[..., 1] = -a[..., 1]\n",
        "    return a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4r4af827HLG"
      },
      "outputs": [],
      "source": [
        "def cconv(a, b):\n",
        "    return torch.irfft(com_mult(torch.rfft(a, 1), torch.rfft(b, 1)), 1, signal_sizes=(a.shape[-1],))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3rH21zN7IrH"
      },
      "outputs": [],
      "source": [
        "def ccorr(a, b):\n",
        "    return torch.irfft(com_mult(conj(torch.rfft(a, 1)), torch.rfft(b, 1)), 1, signal_sizes=(a.shape[-1],))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f5QWVTNfhGY"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D36lnInwfazg"
      },
      "source": [
        "### SpecialSpmmFinal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXzV5SPIfi-6"
      },
      "outputs": [],
      "source": [
        "class SpecialSpmmFunctionFinal(torch.autograd.Function):\n",
        "    \"\"\"Special function for only sparse region backpropataion layer.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, edge, edge_w, size1, size2, out_features, dim):\n",
        "        # assert indices.requires_grad == False\n",
        "        # assert not torch.isnan(edge).any()\n",
        "        # assert not torch.isnan(edge_w).any()\n",
        "        a = torch.sparse_coo_tensor(\n",
        "            edge, edge_w, torch.Size([size1, size2, out_features]))\n",
        "        b = torch.sparse.sum(a, dim=dim)\n",
        "        ctx.size1 = b.shape[0]\n",
        "        ctx.outfeat = b.shape[1]\n",
        "        ctx.size2 = size2\n",
        "        if dim == 0:\n",
        "            ctx.indices = a._indices()[1, :]\n",
        "        else:\n",
        "            ctx.indices = a._indices()[0, :]\n",
        "        return b.to_dense()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        grad_values = None\n",
        "        if ctx.needs_input_grad[1]:\n",
        "            edge_sources = ctx.indices\n",
        "            if torch.cuda.is_available():\n",
        "                edge_sources = edge_sources.cuda()\n",
        "\n",
        "            grad_values = grad_output[edge_sources]\n",
        "            # grad_values = grad_values.view(ctx.E, ctx.outfeat)\n",
        "            # print(\"Grad Outputs-> \", grad_output)\n",
        "            # print(\"Grad values-> \", grad_values)\n",
        "        return None, grad_values, None, None, None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbyQZMTJfnNy"
      },
      "outputs": [],
      "source": [
        "class SpecialSpmmFinal(torch.nn.Module):\n",
        "    def forward(self, edge, edge_w, size1, size2, out_features, dim=1):\n",
        "        return SpecialSpmmFunctionFinal.apply(edge, edge_w, size1, size2, out_features, dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dT_NoSQftCS"
      },
      "source": [
        "### Message Passing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RTgKOe7f5ls"
      },
      "outputs": [],
      "source": [
        "def scatter_(name, src, index, dim_size=None):\n",
        "    r\"\"\"Aggregates all values from the :attr:`src` tensor at the indices\n",
        "    specified in the :attr:`index` tensor along the first dimension.\n",
        "    If multiple indices reference the same location, their contributions\n",
        "    are aggregated according to :attr:`name` (either :obj:`\"add\"`,\n",
        "    :obj:`\"mean\"` or :obj:`\"max\"`).\n",
        "    Args:\n",
        "        name (string): The aggregation to use (:obj:`\"add\"`, :obj:`\"mean\"`,\n",
        "            :obj:`\"max\"`).\n",
        "        src (Tensor): The source tensor.\n",
        "        index (LongTensor): The indices of elements to scatter.\n",
        "        dim_size (int, optional): Automatically create output tensor with size\n",
        "            :attr:`dim_size` in the first dimension. If set to :attr:`None`, a\n",
        "            minimal sized output tensor is returned. (default: :obj:`None`)\n",
        "    :rtype: :class:`Tensor`\n",
        "    \"\"\"\n",
        "    if name == 'add':\n",
        "        name = 'sum'\n",
        "    assert name in ['sum', 'mean', 'max']\n",
        "    spm = SpecialSpmmFinal()\n",
        "    # out = scatter(src, index, dim=0, out=None, dim_size=dim_size, reduce=name)\n",
        "    out = spm((index.cpu().numpy(), list(range(src.shape[0]))), src, dim_size, src.shape[0], src.shape[1], dim=1)\n",
        "    return out[0] if isinstance(out, tuple) else out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5P7qhR5fnwo"
      },
      "outputs": [],
      "source": [
        "class MessagePassing(torch.nn.Module):\n",
        "    r\"\"\"Base class for creating message passing layers\n",
        "    .. math::\n",
        "        \\mathbf{x}_i^{\\prime} = \\gamma_{\\mathbf{\\Theta}} \\left( \\mathbf{x}_i,\n",
        "        \\square_{j \\in \\mathcal{N}(i)} \\, \\phi_{\\mathbf{\\Theta}}\n",
        "        \\left(\\mathbf{x}_i, \\mathbf{x}_j,\\mathbf{e}_{i,j}\\right) \\right),\n",
        "    where :math:`\\square` denotes a differentiable, permutation invariant\n",
        "    function, *e.g.*, sum, mean or max, and :math:`\\gamma_{\\mathbf{\\Theta}}`\n",
        "    and :math:`\\phi_{\\mathbf{\\Theta}}` denote differentiable functions such as\n",
        "    MLPs.\n",
        "    See `here <https://rusty1s.github.io/pytorch_geometric/build/html/notes/\n",
        "    create_gnn.html>`__ for the accompanying tutorial.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, aggr='add'):\n",
        "        super(MessagePassing, self).__init__()\n",
        "        # In the defined message function: get the list of arguments as list of string|\n",
        "        # For eg. in rgcn this will be ['x_j', 'edge_type', 'edge_norm'] (arguments of message function)\n",
        "        self.message_args = inspect.getargspec(self.message)[0][1:]\n",
        "        # Same for update function starting from 3rd argument | first=self, second=out\n",
        "        self.update_args = inspect.getargspec(self.update)[0][2:]\n",
        "\n",
        "    def propagate(self, aggr, edge_index, **kwargs):\n",
        "        r\"\"\"The initial call to start propagating messages.\n",
        "        Takes in an aggregation scheme (:obj:`\"add\"`, :obj:`\"mean\"` or\n",
        "        :obj:`\"max\"`), the edge indices, and all additional data which is\n",
        "        needed to construct messages and to update node embeddings.\"\"\"\n",
        "\n",
        "        assert aggr in ['add', 'mean', 'max']\n",
        "        kwargs['edge_index'] = edge_index\n",
        "\n",
        "        size = None\n",
        "        message_args = []\n",
        "        for arg in self.message_args:\n",
        "            if arg[-2:] == '_i':  # If arguments ends with _i then include indic\n",
        "                tmp = kwargs[arg[:-2]]  # Take the front part of the variable | Mostly it will be 'x',\n",
        "                size = tmp.size(0)\n",
        "                message_args.append(tmp[edge_index[0]])  # Lookup for head entities in edges\n",
        "            elif arg[-2:] == '_j':\n",
        "                tmp = kwargs[arg[:-2]]  # tmp = kwargs['x']\n",
        "                size = tmp.size(0)\n",
        "                message_args.append(tmp[edge_index[1]])  # Lookup for tail entities in edges\n",
        "            else:\n",
        "                message_args.append(kwargs[arg])  # Take things from kwargs\n",
        "\n",
        "        update_args = [kwargs[arg] for arg in self.update_args]  # Take update args from kwargs\n",
        "\n",
        "        out = self.message(*message_args)\n",
        "        if self.p.att is None:\n",
        "            out = scatter_(aggr, out, edge_index[0], dim_size=size)  # Aggregated neighbors for each vertex\n",
        "        out = self.update(out, *update_args)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def message(self, x_j):  # pragma: no cover\n",
        "        r\"\"\"Constructs messages in analogy to :math:`\\phi_{\\mathbf{\\Theta}}`\n",
        "        for each edge in :math:`(i,j) \\in \\mathcal{E}`.\n",
        "        Can take any argument which was initially passed to :meth:`propagate`.\n",
        "        In addition, features can be lifted to the source node :math:`i` and\n",
        "        target node :math:`j` by appending :obj:`_i` or :obj:`_j` to the\n",
        "        variable name, *.e.g.* :obj:`x_i` and :obj:`x_j`.\"\"\"\n",
        "\n",
        "        return x_j\n",
        "\n",
        "    def update(self, aggr_out):  # pragma: no cover\n",
        "        r\"\"\"Updates node embeddings in analogy to\n",
        "        :math:`\\gamma_{\\mathbf{\\Theta}}` for each node\n",
        "        :math:`i \\in \\mathcal{V}`.\n",
        "        Takes in the output of aggregation as first argument and any argument\n",
        "        which was initially passed to :meth:`propagate`.\"\"\"\n",
        "\n",
        "        return aggr_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxPcWbEdgp0t"
      },
      "source": [
        "### RAGAT Conv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slZWnYtDgrxu"
      },
      "outputs": [],
      "source": [
        "class RagatConv(MessagePassing):\n",
        "    def __init__(self, edge_index, edge_type, in_channels, out_channels, num_rels, act=lambda x: x, params=None,\n",
        "                 head_num=1):\n",
        "        super(self.__class__, self).__init__()\n",
        "\n",
        "        self.edge_index = edge_index\n",
        "        self.edge_type = edge_type\n",
        "        self.p = params\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.num_rels = num_rels\n",
        "        self.act = act\n",
        "        self.device = None\n",
        "        self.head_num = head_num\n",
        "\n",
        "        self.w1_loop = get_param((in_channels, out_channels))\n",
        "        self.w1_in = get_param((in_channels, out_channels))\n",
        "        self.w1_out = get_param((in_channels, out_channels))\n",
        "        self.w_rel = get_param((in_channels, out_channels))\n",
        "\n",
        "        if self.p.opn == 'concat' or self.p.opn == 'cross_concat':\n",
        "            self.w1_loop = get_param((2 * in_channels, out_channels))\n",
        "            self.w1_in = get_param((2 * in_channels, out_channels))\n",
        "            self.w1_out = get_param((2 * in_channels, out_channels))\n",
        "\n",
        "        self.loop_rel = get_param((1, in_channels))\n",
        "\n",
        "        self.drop = torch.nn.Dropout(self.p.dropout)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.bn = torch.nn.BatchNorm1d(out_channels)\n",
        "\n",
        "        if self.p.bias:\n",
        "            self.register_parameter('bias', Parameter(torch.zeros(out_channels)))\n",
        "        self.special_spmm = SpecialSpmmFinal()\n",
        "\n",
        "        self.w_att_head1 = get_param((out_channels, 1))\n",
        "\n",
        "        num_edges = self.edge_index.size(1) // 2\n",
        "        if self.device is None:\n",
        "            self.device = self.edge_index.device\n",
        "        self.in_index, self.out_index = self.edge_index[:, :num_edges], self.edge_index[:, num_edges:]\n",
        "        self.in_type, self.out_type = self.edge_type[:num_edges], self.edge_type[num_edges:]\n",
        "        self.loop_index = torch.stack([torch.arange(self.p.num_ent), torch.arange(self.p.num_ent)]).to(self.device)\n",
        "        self.loop_type = torch.full((self.p.num_ent,), 2 * self.num_rels, dtype=torch.long).to(self.device)\n",
        "        # E * 1, norm A\n",
        "        num_ent = self.p.num_ent\n",
        "        self.in_norm = None if self.p.att else self.compute_norm(self.in_index, num_ent)\n",
        "        self.out_norm = None if self.p.att else self.compute_norm(self.out_index, num_ent)\n",
        "\n",
        "        self.leakyrelu = torch.nn.LeakyReLU(0.2)\n",
        "        self.rel_weight1 = get_param((2 * self.num_rels + 1, in_channels))\n",
        "        if self.head_num == 2 or self.head_num == 3:\n",
        "            self.w2_loop = get_param((in_channels, out_channels))\n",
        "            self.w2_in = get_param((in_channels, out_channels))\n",
        "            self.w2_out = get_param((in_channels, out_channels))\n",
        "\n",
        "            if self.p.opn == 'concat' or self.p.opn == 'cross_concat':\n",
        "                self.w2_loop = get_param((2 * in_channels, out_channels))\n",
        "                self.w2_in = get_param((2 * in_channels, out_channels))\n",
        "                self.w2_out = get_param((2 * in_channels, out_channels))\n",
        "            self.w_att_head2 = get_param((out_channels, 1))\n",
        "            self.rel_weight2 = get_param((2 * self.num_rels + 1, in_channels))\n",
        "\n",
        "        if self.head_num == 3:\n",
        "            self.w3_loop = get_param((in_channels, out_channels))\n",
        "            self.w3_in = get_param((in_channels, out_channels))\n",
        "            self.w3_out = get_param((in_channels, out_channels))\n",
        "            if self.p.opn == 'concat' or self.p.opn == 'cross_concat':\n",
        "                self.w3_loop = get_param((2 * in_channels, out_channels))\n",
        "                self.w3_in = get_param((2 * in_channels, out_channels))\n",
        "                self.w3_out = get_param((2 * in_channels, out_channels))\n",
        "            self.w_att_head3 = get_param((out_channels, 1))\n",
        "            self.rel_weight3 = get_param((2 * self.num_rels + 1, in_channels))\n",
        "\n",
        "    def forward(self, x, rel_embed):\n",
        "        rel_embed = torch.cat([rel_embed, self.loop_rel], dim=0)\n",
        "        # 2 * num_ent\n",
        "        in_res1 = self.propagate('add', self.in_index, x=x, edge_type=self.in_type, rel_embed=rel_embed,\n",
        "                                 rel_weight=self.rel_weight1, edge_norm=self.in_norm, mode='in', w_str='w1_{}')\n",
        "        loop_res1 = self.propagate('add', self.loop_index, x=x, edge_type=self.loop_type, rel_embed=rel_embed,\n",
        "                                   rel_weight=self.rel_weight1, edge_norm=None, mode='loop', w_str='w1_{}')\n",
        "        out_res1 = self.propagate('add', self.out_index, x=x, edge_type=self.out_type, rel_embed=rel_embed,\n",
        "                                  rel_weight=self.rel_weight1, edge_norm=self.out_norm, mode='out', w_str='w1_{}')\n",
        "        if self.head_num == 2 or self.head_num == 3:\n",
        "            in_res2 = self.propagate('add', self.in_index, x=x, edge_type=self.in_type, rel_embed=rel_embed,\n",
        "                                     rel_weight=self.rel_weight2, edge_norm=self.in_norm, mode='in', w_str='w2_{}')\n",
        "            loop_res2 = self.propagate('add', self.loop_index, x=x, edge_type=self.loop_type, rel_embed=rel_embed,\n",
        "                                       rel_weight=self.rel_weight2, edge_norm=None, mode='loop', w_str='w2_{}')\n",
        "            out_res2 = self.propagate('add', self.out_index, x=x, edge_type=self.out_type, rel_embed=rel_embed,\n",
        "                                      rel_weight=self.rel_weight2, edge_norm=self.out_norm, mode='out', w_str='w2_{}')\n",
        "        if self.head_num == 3:\n",
        "            in_res3 = self.propagate('add', self.in_index, x=x, edge_type=self.in_type, rel_embed=rel_embed,\n",
        "                                     rel_weight=self.rel_weight3, edge_norm=self.in_norm, mode='in', w_str='w3_{}')\n",
        "            loop_res3 = self.propagate('add', self.loop_index, x=x, edge_type=self.loop_type, rel_embed=rel_embed,\n",
        "                                       rel_weight=self.rel_weight3, edge_norm=None, mode='loop', w_str='w3_{}')\n",
        "            out_res3 = self.propagate('add', self.out_index, x=x, edge_type=self.out_type, rel_embed=rel_embed,\n",
        "                                      rel_weight=self.rel_weight3, edge_norm=self.out_norm, mode='out', w_str='w3_{}')\n",
        "        if self.p.att:\n",
        "            out1 = self.agg_multi_head(in_res1, out_res1, loop_res1, 1)\n",
        "            if self.head_num == 2:\n",
        "                out2 = self.agg_multi_head(in_res2, out_res2, loop_res2, 2)\n",
        "                out = 1 / 2 * (out1 + out2)\n",
        "            elif self.head_num == 3:\n",
        "                out2 = self.agg_multi_head(in_res2, out_res2, loop_res2, 2)\n",
        "                out3 = self.agg_multi_head(in_res3, out_res3, loop_res3, 3)\n",
        "                out = 1 / 3 * (out1 + out2 + out3)\n",
        "            else:\n",
        "                out = out1\n",
        "        else:\n",
        "            out = self.drop(in_res1) * (1 / 3) + self.drop(out_res1) * (1 / 3) + loop_res1 * (1 / 3)\n",
        "        if self.p.bias:\n",
        "            out = out + self.bias\n",
        "        relation1 = rel_embed.mm(self.w_rel)\n",
        "        out = self.bn(out)\n",
        "        entity1 = self.act(out)\n",
        "\n",
        "        return entity1, relation1[:-1]\n",
        "\n",
        "    def agg_multi_head(self, in_res, out_res, loop_res, head_no):\n",
        "        att_weight = getattr(self, 'w_att_head{}'.format(head_no))\n",
        "        edge_index = torch.cat([self.edge_index, self.loop_index], dim=1)\n",
        "        all_message = torch.cat([in_res, out_res, loop_res], dim=0)\n",
        "        powers = -self.leakyrelu(all_message.mm(att_weight).squeeze())\n",
        "        # edge_exp: E * 1\n",
        "        edge_exp = torch.exp(powers).unsqueeze(1)\n",
        "        weight_rowsum = self.special_spmm(\n",
        "            edge_index, edge_exp, self.p.num_ent, self.p.num_ent, 1, dim=1)\n",
        "        # except 0\n",
        "        weight_rowsum[weight_rowsum == 0.0] = 1.0\n",
        "        # weight_rowsum: num_nodes x 1\n",
        "        # info_emb_weighted: E * D\n",
        "        edge_exp = self.drop(edge_exp)\n",
        "        info_emb_weighted = edge_exp * all_message\n",
        "        # assert not torch.isnan(info_emb_weighted).any()\n",
        "        emb_agg = self.special_spmm(\n",
        "            edge_index, info_emb_weighted, self.p.num_ent, self.p.num_ent, all_message.shape[1], dim=1)\n",
        "        # emb_agg: N x D, finish softmax\n",
        "        emb_agg = emb_agg.div(weight_rowsum)\n",
        "        assert not torch.isnan(emb_agg).any()\n",
        "        return emb_agg\n",
        "\n",
        "    def rel_transform(self, ent_embed, rel_embed, rel_weight, opn=None):\n",
        "        if opn is None:\n",
        "            opn = self.p.opn\n",
        "        if opn == 'corr':\n",
        "            trans_embed = ccorr(ent_embed, rel_embed)\n",
        "        elif opn == 'corr_ra':\n",
        "            trans_embed = ccorr(ent_embed * rel_weight, rel_embed)\n",
        "        elif opn == 'sub':\n",
        "            trans_embed = ent_embed - rel_embed\n",
        "        elif opn == 'es':\n",
        "            trans_embed = ent_embed\n",
        "        elif opn == 'sub_ra':\n",
        "            trans_embed = ent_embed * rel_weight - rel_embed\n",
        "        elif opn == 'mult':\n",
        "            trans_embed = ent_embed * rel_embed\n",
        "        elif opn == 'mult_ra':\n",
        "            trans_embed = (ent_embed * rel_embed) * rel_weight\n",
        "        elif opn == 'cross':\n",
        "            trans_embed = ent_embed * rel_embed * rel_weight + ent_embed * rel_weight\n",
        "        elif opn == 'cross_wo_rel':\n",
        "            trans_embed = ent_embed * rel_weight\n",
        "        elif opn == 'cross_simplfy':\n",
        "            trans_embed = ent_embed * rel_embed + ent_embed\n",
        "        elif opn == 'concat':\n",
        "            trans_embed = torch.cat([ent_embed, rel_embed], dim=1)\n",
        "        elif opn == 'concat_ra':\n",
        "            trans_embed = torch.cat([ent_embed, rel_embed], dim=1) * rel_weight\n",
        "        elif opn == 'ent_ra':\n",
        "            trans_embed = ent_embed * rel_weight + rel_embed\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        return trans_embed\n",
        "\n",
        "    def message(self, x_j, edge_type, rel_embed, rel_weight, edge_norm, mode, w_str):\n",
        "        weight = getattr(self, w_str.format(mode))\n",
        "        rel_emb = torch.index_select(rel_embed, 0, edge_type)\n",
        "        rel_weight = torch.index_select(rel_weight, 0, edge_type)\n",
        "        xj_rel = self.rel_transform(x_j, rel_emb, rel_weight)\n",
        "        out = torch.mm(xj_rel, weight)\n",
        "        assert not torch.isnan(out).any()\n",
        "        return out if edge_norm is None else out * edge_norm.view(-1, 1)\n",
        "\n",
        "    def update(self, aggr_out):\n",
        "        return aggr_out\n",
        "\n",
        "    def compute_norm(self, edge_index, num_ent):\n",
        "        row, col = edge_index\n",
        "        edge_weight = torch.ones_like(row).float().unsqueeze(1)\n",
        "        deg = self.special_spmm((row.cpu().numpy(), col.cpu().numpy()), edge_weight, num_ent, num_ent, 1, dim=1)\n",
        "        deg_inv = deg.pow(-0.5)  # D^{-0.5}\n",
        "        deg_inv[deg_inv == float('inf')] = 0\n",
        "        norm = deg_inv[row] * edge_weight * deg_inv[col]  # D^{-0.5}\n",
        "\n",
        "        return norm\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}({}, {}, num_rels={})'.format(\n",
        "            self.__class__.__name__, self.in_channels, self.out_channels, self.num_rels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yNK8UepgI2H"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrdVip8mgSAb"
      },
      "source": [
        "#### Base Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pa09eZj2gKtg"
      },
      "outputs": [],
      "source": [
        "class BaseModel(torch.nn.Module):\n",
        "    def __init__(self, params):\n",
        "        super(BaseModel, self).__init__()\n",
        "\n",
        "        self.p = params\n",
        "        self.act = torch.tanh\n",
        "        self.bceloss = torch.nn.BCELoss()\n",
        "\n",
        "    def loss(self, pred, true_label):\n",
        "        return self.bceloss(pred, true_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oE2v9XA-gToK"
      },
      "source": [
        "#### RAGAT Base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuUOOOy6gOYP"
      },
      "outputs": [],
      "source": [
        "class RagatBase(BaseModel):\n",
        "    def __init__(self, edge_index, edge_type, num_rel, params=None):\n",
        "        #super(RagatBase, self).__init__(params)\n",
        "        super().__init__(params)\n",
        "\n",
        "        self.edge_index = edge_index\n",
        "        self.edge_type = edge_type\n",
        "        self.p.gcn_dim = self.p.embed_dim if self.p.gcn_layer == 1 else self.p.gcn_dim\n",
        "        self.init_embed = get_param((self.p.num_ent, self.p.init_dim))\n",
        "        self.device = self.edge_index.device\n",
        "\n",
        "        if self.p.score_func == 'transe':\n",
        "            self.init_rel = get_param((num_rel, self.p.init_dim))\n",
        "        else:\n",
        "            self.init_rel = get_param((num_rel * 2, self.p.init_dim))\n",
        "\n",
        "        self.conv1 = RagatConv(self.edge_index, self.edge_type, self.p.init_dim, self.p.gcn_dim, num_rel,\n",
        "                               act=self.act, params=self.p, head_num=self.p.head_num)\n",
        "        self.conv2 = RagatConv(self.edge_index, self.edge_type, self.p.gcn_dim, self.p.embed_dim, num_rel,\n",
        "                               act=self.act, params=self.p, head_num=1) if self.p.gcn_layer == 2 else None\n",
        "\n",
        "        self.register_parameter('bias', Parameter(torch.zeros(self.p.num_ent)))\n",
        "        self.special_spmm = SpecialSpmmFinal()\n",
        "        self.rel_drop = torch.nn.Dropout(0.1)\n",
        "\n",
        "    def forward_base(self, sub, rel, drop1, drop2):\n",
        "        # r: (2 * num_relation) x init_dim\n",
        "        init_rel = self.init_rel if self.p.score_func != 'transe' else torch.cat([self.init_rel, -self.init_rel], dim=0)\n",
        "        ent_embed1, rel_embed1 = self.conv1(x=self.init_embed, rel_embed=init_rel)\n",
        "        ent_embed1 = drop1(ent_embed1)\n",
        "\n",
        "        ent_embed2, rel_embed2 = self.conv2(x=ent_embed1, rel_embed=rel_embed1) if self.p.gcn_layer == 2 else (\n",
        "            ent_embed1, rel_embed1)\n",
        "        ent_embed2 = drop2(ent_embed2) if self.p.gcn_layer == 2 else ent_embed1\n",
        "\n",
        "        final_ent = ent_embed2 if self.p.gcn_layer == 2 else ent_embed1\n",
        "        final_rel = rel_embed2 if self.p.gcn_layer == 2 else rel_embed1\n",
        "        sub_emb = torch.index_select(final_ent, 0, sub)\n",
        "        rel_emb = torch.index_select(final_rel, 0, rel)\n",
        "        return sub_emb, rel_emb, final_ent\n",
        "\n",
        "    def gather_neighbours(self):\n",
        "        edge_weight = torch.ones_like(self.edge_type).float().unsqueeze(1)\n",
        "        deg = self.special_spmm(self.edge_index, edge_weight, self.p.num_ent, self.p.num_ent, 1,\n",
        "                                dim=1)\n",
        "        deg[deg == 0.0] = 1.0\n",
        "        entity_neighbours = self.init_embed[self.edge_index[1, :], :]\n",
        "        entity_gathered = self.special_spmm(\n",
        "            self.edge_index, entity_neighbours, self.p.num_ent, self.p.num_ent, self.p.init_dim,\n",
        "            dim=1).div(deg)\n",
        "        relation_neighbours = torch.index_select(self.init_rel, 0, self.edge_type)\n",
        "        relation_gathered = self.special_spmm(\n",
        "            self.edge_index, relation_neighbours, self.p.num_ent, self.p.num_ent, self.p.init_dim, dim=1).div(deg)\n",
        "        return entity_gathered, relation_gathered"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmEK5xjDgXOz"
      },
      "source": [
        "#### RAGAT TransE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDR8fmrLgbtr"
      },
      "outputs": [],
      "source": [
        "class RagatTransE(RagatBase):\n",
        "    def __init__(self, edge_index, edge_type, params=None):\n",
        "        super(self.__class__, self).__init__(edge_index, edge_type, params.num_rel, params)\n",
        "        self.drop = torch.nn.Dropout(self.p.hid_drop)\n",
        "\n",
        "    def forward(self, sub, rel):\n",
        "        sub_emb, rel_emb, all_ent = self.forward_base(sub, rel, self.drop, self.drop)\n",
        "        obj_emb = sub_emb + rel_emb\n",
        "\n",
        "        x = self.p.gamma - torch.norm(obj_emb.unsqueeze(1) - all_ent, p=1, dim=2)\n",
        "        score = torch.sigmoid(x)\n",
        "\n",
        "        return score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o1okaZwgbKi"
      },
      "source": [
        "#### RAGAT DistMult"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blA2UaHPgfV1"
      },
      "outputs": [],
      "source": [
        "class RagatDistMult(RagatBase):\n",
        "    def __init__(self, edge_index, edge_type, params=None):\n",
        "        super(self.__class__, self).__init__(edge_index, edge_type, params.num_rel, params)\n",
        "        self.drop = torch.nn.Dropout(self.p.hid_drop)\n",
        "\n",
        "    def forward(self, sub, rel):\n",
        "        sub_emb, rel_emb, all_ent = self.forward_base(sub, rel, self.drop, self.drop)\n",
        "        obj_emb = sub_emb * rel_emb\n",
        "\n",
        "        x = torch.mm(obj_emb, all_ent.transpose(1, 0))\n",
        "        x += self.bias.expand_as(x)\n",
        "\n",
        "        score = torch.sigmoid(x)\n",
        "        return "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcT_eLVtgfxJ"
      },
      "source": [
        "#### RAGAT ConvE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TPByQc6gjQ5"
      },
      "outputs": [],
      "source": [
        "class RagatConvE(RagatBase):\n",
        "    def __init__(self, edge_index, edge_type, params=None):\n",
        "        super(self.__class__, self).__init__(edge_index, edge_type, params.num_rel, params)\n",
        "        self.embed_dim = self.p.embed_dim\n",
        "\n",
        "        self.bn0 = torch.nn.BatchNorm2d(1)\n",
        "        self.bn1 = torch.nn.BatchNorm2d(self.p.num_filt)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(self.embed_dim)\n",
        "\n",
        "        self.hidden_drop = torch.nn.Dropout(self.p.hid_drop)\n",
        "        self.hidden_drop2 = torch.nn.Dropout(self.p.hid_drop2)\n",
        "        self.feature_drop = torch.nn.Dropout(self.p.feat_drop)\n",
        "        self.m_conv1 = torch.nn.Conv2d(1, out_channels=self.p.num_filt, kernel_size=(self.p.ker_sz, self.p.ker_sz),\n",
        "                                       stride=1, padding=0, bias=self.p.bias)\n",
        "\n",
        "        flat_sz_h = int(2 * self.p.k_w) - self.p.ker_sz + 1\n",
        "        flat_sz_w = self.p.k_h - self.p.ker_sz + 1\n",
        "        self.flat_sz = flat_sz_h * flat_sz_w * self.p.num_filt\n",
        "        self.fc = torch.nn.Linear(self.flat_sz, self.embed_dim)\n",
        "\n",
        "    def concat(self, e1_embed, rel_embed):\n",
        "        e1_embed = e1_embed.view(-1, 1, self.embed_dim)\n",
        "        rel_embed = rel_embed.view(-1, 1, self.embed_dim)\n",
        "        stack_inp = torch.cat([e1_embed, rel_embed], 1)\n",
        "        stack_inp = torch.transpose(stack_inp, 2, 1).reshape((-1, 1, 2 * self.p.k_w, self.p.k_h))\n",
        "        return stack_inp\n",
        "\n",
        "    def forward(self, sub, rel, neg_ents=None):\n",
        "        sub_emb, rel_emb, all_ent = self.forward_base(sub, rel, self.hidden_drop, self.feature_drop)\n",
        "        stk_inp = self.concat(sub_emb, rel_emb)\n",
        "        x = self.bn0(stk_inp)\n",
        "        x = self.m_conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.feature_drop(x)\n",
        "        x = x.view(-1, self.flat_sz)\n",
        "        x = self.fc(x)\n",
        "        x = self.hidden_drop2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = torch.mm(x, all_ent.transpose(1, 0))\n",
        "        x += self.bias.expand_as(x)\n",
        "\n",
        "        score = torch.sigmoid(x)\n",
        "        return score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTIGytQ8gj0g"
      },
      "source": [
        "#### RAGAT IntractE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-716vkL0gllX"
      },
      "outputs": [],
      "source": [
        "class RagatInteractE(RagatBase):\n",
        "    def __init__(self, edge_index, edge_type, params=None):\n",
        "        super(self.__class__, self).__init__(edge_index, edge_type, params.num_rel, params)\n",
        "        # self.ent_embed = torch.nn.Embedding(self.p.num_ent, self.p.embed_dim, padding_idx=None)\n",
        "        # xavier_normal_(self.ent_embed.weight)\n",
        "        # self.rel_embed = torch.nn.Embedding(self.p.num_rel * 2, self.p.embed_dim, padding_idx=None)\n",
        "        # xavier_normal_(self.rel_embed.weight)\n",
        "\n",
        "        self.inp_drop = torch.nn.Dropout(self.p.iinp_drop)\n",
        "        self.feature_map_drop = torch.nn.Dropout2d(self.p.ifeat_drop)\n",
        "        self.hidden_drop = torch.nn.Dropout(self.p.ihid_drop)\n",
        "\n",
        "        self.hidden_drop_gcn = torch.nn.Dropout(0)\n",
        "\n",
        "        self.bn0 = torch.nn.BatchNorm2d(self.p.iperm)\n",
        "\n",
        "        flat_sz_h = self.p.ik_h\n",
        "        flat_sz_w = 2 * self.p.ik_w\n",
        "        self.padding = 0\n",
        "\n",
        "        self.bn1 = torch.nn.BatchNorm2d(self.p.inum_filt * self.p.iperm)\n",
        "        self.flat_sz = flat_sz_h * flat_sz_w * self.p.inum_filt * self.p.iperm\n",
        "\n",
        "        self.bn2 = torch.nn.BatchNorm1d(self.p.embed_dim)\n",
        "        self.fc = torch.nn.Linear(self.flat_sz, self.p.embed_dim)\n",
        "        self.chequer_perm = self.get_chequer_perm()\n",
        "\n",
        "        self.register_parameter('bias', Parameter(torch.zeros(self.p.num_ent)))\n",
        "        self.register_parameter('conv_filt',\n",
        "                                Parameter(torch.zeros(self.p.inum_filt, 1, self.p.iker_sz, self.p.iker_sz)))\n",
        "        xavier_normal_(self.conv_filt)\n",
        "\n",
        "    def circular_padding_chw(self, batch, padding):\n",
        "        upper_pad = batch[..., -padding:, :]\n",
        "        lower_pad = batch[..., :padding, :]\n",
        "        temp = torch.cat([upper_pad, batch, lower_pad], dim=2)\n",
        "\n",
        "        left_pad = temp[..., -padding:]\n",
        "        right_pad = temp[..., :padding]\n",
        "        padded = torch.cat([left_pad, temp, right_pad], dim=3)\n",
        "        return padded\n",
        "\n",
        "    def forward(self, sub, rel, neg_ents=None):\n",
        "        sub_emb, rel_emb, all_ent = self.forward_base(sub, rel, self.inp_drop, self.hidden_drop_gcn)\n",
        "        comb_emb = torch.cat([sub_emb, rel_emb], dim=1)\n",
        "        chequer_perm = comb_emb[:, self.chequer_perm]\n",
        "        stack_inp = chequer_perm.reshape((-1, self.p.iperm, 2 * self.p.ik_w, self.p.ik_h))\n",
        "        stack_inp = self.bn0(stack_inp)\n",
        "        x = stack_inp\n",
        "        x = self.circular_padding_chw(x, self.p.iker_sz // 2)\n",
        "        x = F.conv2d(x, self.conv_filt.repeat(self.p.iperm, 1, 1, 1), padding=self.padding, groups=self.p.iperm)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.feature_map_drop(x)\n",
        "        x = x.view(-1, self.flat_sz)\n",
        "        x = self.fc(x)\n",
        "        x = self.hidden_drop(x)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        if self.p.strategy == 'one_to_n' or neg_ents is None:\n",
        "            x = torch.mm(x, all_ent.transpose(1, 0))\n",
        "            x += self.bias.expand_as(x)\n",
        "        else:\n",
        "            x = torch.mul(x.unsqueeze(1), all_ent[neg_ents]).sum(dim=-1)\n",
        "            x += self.bias[neg_ents]\n",
        "\n",
        "        pred = torch.sigmoid(x)\n",
        "\n",
        "        return pred\n",
        "\n",
        "    def get_chequer_perm(self):\n",
        "        \"\"\"\n",
        "        Function to generate the chequer permutation required for InteractE model\n",
        "        Parameters\n",
        "        ----------\n",
        "        Returns\n",
        "        -------\n",
        "        \"\"\"\n",
        "        ent_perm = np.int32([np.random.permutation(self.p.embed_dim) for _ in range(self.p.iperm)])\n",
        "        rel_perm = np.int32([np.random.permutation(self.p.embed_dim) for _ in range(self.p.iperm)])\n",
        "\n",
        "        comb_idx = []\n",
        "        for k in range(self.p.iperm):\n",
        "            temp = []\n",
        "            ent_idx, rel_idx = 0, 0\n",
        "\n",
        "            for i in range(self.p.ik_h):\n",
        "                for j in range(self.p.ik_w):\n",
        "                    if k % 2 == 0:\n",
        "                        if i % 2 == 0:\n",
        "                            temp.append(ent_perm[k, ent_idx])\n",
        "                            ent_idx += 1\n",
        "                            temp.append(rel_perm[k, rel_idx] + self.p.embed_dim)\n",
        "                            rel_idx += 1\n",
        "                        else:\n",
        "                            temp.append(rel_perm[k, rel_idx] + self.p.embed_dim)\n",
        "                            rel_idx += 1\n",
        "                            temp.append(ent_perm[k, ent_idx])\n",
        "                            ent_idx += 1\n",
        "                    else:\n",
        "                        if i % 2 == 0:\n",
        "                            temp.append(rel_perm[k, rel_idx] + self.p.embed_dim)\n",
        "                            rel_idx += 1\n",
        "                            temp.append(ent_perm[k, ent_idx])\n",
        "                            ent_idx += 1\n",
        "                        else:\n",
        "                            temp.append(ent_perm[k, ent_idx])\n",
        "                            ent_idx += 1\n",
        "                            temp.append(rel_perm[k, rel_idx] + self.p.embed_dim)\n",
        "                            rel_idx += 1\n",
        "\n",
        "            comb_idx.append(temp)\n",
        "\n",
        "        chequer_perm = torch.LongTensor(np.int32(comb_idx)).to(self.device)\n",
        "        return chequer_perm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcquDhCB3v2r"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQn2Ljcazl-V"
      },
      "outputs": [],
      "source": [
        "class Runner(object):\n",
        "    def __init__(self, params):\n",
        "        \"\"\"\n",
        "        Constructor of the runner class\n",
        "        Parameters\n",
        "        ----------\n",
        "        params:         List of hyper-parameters of the model\n",
        "        Returns\n",
        "        -------\n",
        "        Creates computational graph and optimizer\n",
        "        \"\"\"\n",
        "        self.p = params\n",
        "        self.logger = get_logger(self.p.name, self.p.log_dir, self.p.config_dir)\n",
        "\n",
        "        self.logger.info(vars(self.p))\n",
        "        pprint(vars(self.p))\n",
        "\n",
        "        if self.p.gpu != '-1' and torch.cuda.is_available():\n",
        "            self.device = torch.device('cuda')\n",
        "            torch.cuda.set_rng_state(torch.cuda.get_rng_state())\n",
        "            torch.backends.cudnn.deterministic = True\n",
        "        else:\n",
        "            self.device = torch.device('cpu')\n",
        "\n",
        "        self.load_data()\n",
        "        self.model = self.add_model(self.p.model, self.p.score_func)\n",
        "        self.optimizer = self.add_optimizer(self.model.parameters())\n",
        "\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"\n",
        "        Reading in raw triples and converts it into a standard format.\n",
        "        Parameters\n",
        "        ----------\n",
        "        self.p.dataset:         Takes in the name of the dataset (FB15k-237)\n",
        "        Returns\n",
        "        -------\n",
        "        self.ent2id:            Entity to unique identifier mapping\n",
        "        self.id2rel:            Inverse mapping of self.ent2id\n",
        "        self.rel2id:            Relation to unique identifier mapping\n",
        "        self.num_ent:           Number of entities in the Knowledge graph\n",
        "        self.num_rel:           Number of relations in the Knowledge graph\n",
        "        self.embed_dim:         Embedding dimension used\n",
        "        self.data['train']:     Stores the triples corresponding to training dataset\n",
        "        self.data['valid']:     Stores the triples corresponding to validation dataset\n",
        "        self.data['test']:      Stores the triples corresponding to test dataset\n",
        "        self.data_iter:\t\tThe dataloader for different data splits\n",
        "        \"\"\"\n",
        "\n",
        "        ent_set, rel_set = OrderedSet(), OrderedSet()\n",
        "        for split in ['train', 'test', 'valid']:\n",
        "            for line in open('./data/{}/{}.txt'.format(self.p.dataset, split)):\n",
        "                sub, rel, obj = map(str.lower, line.strip().split('\\t'))\n",
        "                ent_set.add(sub)\n",
        "                rel_set.add(rel)\n",
        "                ent_set.add(obj)\n",
        "\n",
        "        self.ent2id = {ent: idx for idx, ent in enumerate(ent_set)}\n",
        "        self.rel2id = {rel: idx for idx, rel in enumerate(rel_set)}\n",
        "        self.rel2id.update({rel + '_reverse': idx + len(self.rel2id) for idx, rel in enumerate(rel_set)})\n",
        "\n",
        "        self.id2ent = {idx: ent for ent, idx in self.ent2id.items()}\n",
        "        self.id2rel = {idx: rel for rel, idx in self.rel2id.items()}\n",
        "\n",
        "        self.p.num_ent = len(self.ent2id)\n",
        "        self.p.num_rel = len(self.rel2id) // 2\n",
        "        self.p.embed_dim = self.p.k_w * self.p.k_h if self.p.embed_dim is None else self.p.embed_dim\n",
        "\n",
        "        self.data = ddict(list)\n",
        "        sr2o = ddict(set)\n",
        "\n",
        "        for split in ['train', 'test', 'valid']:\n",
        "            for line in open('./data/{}/{}.txt'.format(self.p.dataset, split)):\n",
        "                sub, rel, obj = map(str.lower, line.strip().split('\\t'))\n",
        "                sub, rel, obj = self.ent2id[sub], self.rel2id[rel], self.ent2id[obj]\n",
        "                self.data[split].append((sub, rel, obj))\n",
        "\n",
        "                if split == 'train':\n",
        "                    sr2o[(sub, rel)].add(obj)\n",
        "                    sr2o[(obj, rel + self.p.num_rel)].add(sub)\n",
        "        # self.data: all origin train + valid + test triplets\n",
        "        self.data = dict(self.data)\n",
        "        # self.sr2o: train origin edges and reverse edges\n",
        "        self.sr2o = {k: list(v) for k, v in sr2o.items()}\n",
        "        for split in ['test', 'valid']:\n",
        "            for sub, rel, obj in self.data[split]:\n",
        "                sr2o[(sub, rel)].add(obj)\n",
        "                sr2o[(obj, rel + self.p.num_rel)].add(sub)\n",
        "\n",
        "        self.sr2o_all = {k: list(v) for k, v in sr2o.items()}\n",
        "        self.triples = ddict(list)\n",
        "\n",
        "        # for (sub, rel), obj in self.sr2o.items():\n",
        "        #     self.triples['train'].append({'triple': (sub, rel, -1), 'label': self.sr2o[(sub, rel)], 'sub_samp': 1})\n",
        "        if self.p.strategy == 'one_to_n':\n",
        "            for (sub, rel), obj in self.sr2o.items():\n",
        "                self.triples['train'].append({'triple': (sub, rel, -1), 'label': self.sr2o[(sub, rel)], 'sub_samp': 1})\n",
        "        else:\n",
        "            for sub, rel, obj in self.data['train']:\n",
        "                rel_inv = rel + self.p.num_rel\n",
        "                sub_samp = len(self.sr2o[(sub, rel)]) + len(self.sr2o[(obj, rel_inv)])\n",
        "                sub_samp = np.sqrt(1 / sub_samp)\n",
        "\n",
        "                self.triples['train'].append(\n",
        "                    {'triple': (sub, rel, obj), 'label': self.sr2o[(sub, rel)], 'sub_samp': sub_samp})\n",
        "                self.triples['train'].append(\n",
        "                    {'triple': (obj, rel_inv, sub), 'label': self.sr2o[(obj, rel_inv)], 'sub_samp': sub_samp})\n",
        "\n",
        "        for split in ['test', 'valid']:\n",
        "            for sub, rel, obj in self.data[split]:\n",
        "                rel_inv = rel + self.p.num_rel\n",
        "                self.triples['{}_{}'.format(split, 'tail')].append(\n",
        "                    {'triple': (sub, rel, obj), 'label': self.sr2o_all[(sub, rel)]})\n",
        "                self.triples['{}_{}'.format(split, 'head')].append(\n",
        "                    {'triple': (obj, rel_inv, sub), 'label': self.sr2o_all[(obj, rel_inv)]})\n",
        "\n",
        "        self.triples = dict(self.triples)\n",
        "\n",
        "        def get_data_loader(dataset_class, split, batch_size, shuffle=True):\n",
        "            return DataLoader(\n",
        "                dataset_class(self.triples[split], self.p),\n",
        "                batch_size=batch_size,\n",
        "                shuffle=shuffle,\n",
        "                num_workers=max(0, self.p.num_workers),\n",
        "                collate_fn=dataset_class.collate_fn\n",
        "            )\n",
        "\n",
        "        self.data_iter = {\n",
        "            'train': get_data_loader(TrainDataset, 'train', self.p.batch_size),\n",
        "            'valid_head': get_data_loader(TestDataset, 'valid_head', self.p.test_batch_size),\n",
        "            'valid_tail': get_data_loader(TestDataset, 'valid_tail', self.p.test_batch_size),\n",
        "            'test_head': get_data_loader(TestDataset, 'test_head', self.p.test_batch_size),\n",
        "            'test_tail': get_data_loader(TestDataset, 'test_tail', self.p.test_batch_size),\n",
        "        }\n",
        "\n",
        "        self.edge_index, self.edge_type = self.construct_adj()\n",
        "\n",
        "    def construct_adj(self):\n",
        "        \"\"\"\n",
        "        Constructor of the runner class\n",
        "        Parameters\n",
        "        ----------\n",
        "        Returns\n",
        "        -------\n",
        "        Constructs the adjacency matrix for GCN\n",
        "        \"\"\"\n",
        "        edge_index, edge_type = [], []\n",
        "\n",
        "        for sub, rel, obj in self.data['train']:\n",
        "            edge_index.append((sub, obj))\n",
        "            edge_type.append(rel)\n",
        "\n",
        "        # Adding inverse edges\n",
        "        for sub, rel, obj in self.data['train']:\n",
        "            edge_index.append((obj, sub))\n",
        "            edge_type.append(rel + self.p.num_rel)\n",
        "        # edge_index: 2 * 2E, edge_type: 2E * 1\n",
        "        edge_index = torch.LongTensor(edge_index).to(self.device).t()\n",
        "        edge_type = torch.LongTensor(edge_type).to(self.device)\n",
        "\n",
        "        return edge_index, edge_type\n",
        "\n",
        "    def add_model(self, model, score_func):\n",
        "        \"\"\"\n",
        "        Creates the computational graph\n",
        "        Parameters\n",
        "        ----------\n",
        "        model_name:     Contains the model name to be created\n",
        "        Returns\n",
        "        -------\n",
        "        Creates the computational graph for model and initializes it\n",
        "        \"\"\"\n",
        "        model_name = '{}_{}'.format(model, score_func)\n",
        "\n",
        "        if model_name.lower() == 'ragat_transe':\n",
        "            model = RagatTransE(self.edge_index, self.edge_type, params=self.p)\n",
        "        elif model_name.lower() == 'ragat_distmult':\n",
        "            model = RagatDistMult(self.edge_index, self.edge_type, params=self.p)\n",
        "        elif model_name.lower() == 'ragat_conve':\n",
        "            model = RagatConvE(self.edge_index, self.edge_type, params=self.p)\n",
        "        elif model_name.lower() == 'ragat_interacte':\n",
        "            model = RagatInteractE(self.edge_index, self.edge_type, params=self.p)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        model.to(self.device)\n",
        "        return model\n",
        "\n",
        "    def add_optimizer(self, parameters):\n",
        "        \"\"\"\n",
        "        Creates an optimizer for training the parameters\n",
        "        Parameters\n",
        "        ----------\n",
        "        parameters:         The parameters of the model\n",
        "        Returns\n",
        "        -------\n",
        "        Returns an optimizer for learning the parameters of the model\n",
        "        \"\"\"\n",
        "        return torch.optim.Adam(parameters, lr=self.p.lr, weight_decay=self.p.l2)\n",
        "\n",
        "    def read_batch(self, batch, split):\n",
        "        \"\"\"\n",
        "        Function to read a batch of data and move the tensors in batch to CPU/GPU\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch: \t\tthe batch to process\n",
        "        split: (string) If split == 'train', 'valid' or 'test' split\n",
        "        Returns\n",
        "        -------\n",
        "        Head, Relation, Tails, labels\n",
        "        \"\"\"\n",
        "        # if split == 'train':\n",
        "        #     triple, label = [_.to(self.device) for _ in batch]\n",
        "        #     return triple[:, 0], triple[:, 1], triple[:, 2], label\n",
        "        # else:\n",
        "        #     triple, label = [_.to(self.device) for _ in batch]\n",
        "        #     return triple[:, 0], triple[:, 1], triple[:, 2], label\n",
        "        if split == 'train':\n",
        "            if self.p.strategy == 'one_to_x':\n",
        "                triple, label, neg_ent, sub_samp = [_.to(self.device) for _ in batch]\n",
        "                return triple[:, 0], triple[:, 1], triple[:, 2], label, neg_ent, sub_samp\n",
        "            else:\n",
        "                triple, label = [_.to(self.device) for _ in batch]\n",
        "                return triple[:, 0], triple[:, 1], triple[:, 2], label, None, None\n",
        "        else:\n",
        "            triple, label = [_.to(self.device) for _ in batch]\n",
        "            return triple[:, 0], triple[:, 1], triple[:, 2], label\n",
        "\n",
        "    def save_model(self, save_path):\n",
        "        \"\"\"\n",
        "        Function to save a model. It saves the model parameters, best validation scores,\n",
        "        best epoch corresponding to best validation, state of the optimizer and all arguments for the run.\n",
        "        Parameters\n",
        "        ----------\n",
        "        save_path: path where the model is saved\n",
        "        Returns\n",
        "        -------\n",
        "        \"\"\"\n",
        "        state = {\n",
        "            'state_dict': self.model.state_dict(),\n",
        "            'best_val': self.best_val,\n",
        "            'best_epoch': self.best_epoch,\n",
        "            'optimizer': self.optimizer.state_dict(),\n",
        "            'args': vars(self.p)\n",
        "        }\n",
        "        torch.save(state, save_path)\n",
        "\n",
        "    def load_model(self, load_path):\n",
        "        \"\"\"\n",
        "        Function to load a saved model\n",
        "        Parameters\n",
        "        ----------\n",
        "        load_path: path to the saved model\n",
        "        Returns\n",
        "        -------\n",
        "        \"\"\"\n",
        "        state = torch.load(load_path)\n",
        "        state_dict = state['state_dict']\n",
        "        self.best_val = state['best_val']\n",
        "        self.best_val_mrr = self.best_val['mrr']\n",
        "\n",
        "        self.model.load_state_dict(state_dict)\n",
        "        self.optimizer.load_state_dict(state['optimizer'])\n",
        "\n",
        "    def evaluate(self, split, epoch):\n",
        "        \"\"\"\n",
        "        Function to evaluate the model on validation or test set\n",
        "        Parameters\n",
        "        ----------\n",
        "        split: (string) If split == 'valid' then evaluate on the validation set, else the test set\n",
        "        epoch: (int) Current epoch count\n",
        "        Returns\n",
        "        -------\n",
        "        resutls:\t\t\tThe evaluation results containing the following:\n",
        "            results['mr']:         \tAverage of ranks_left and ranks_right\n",
        "            results['mrr']:         Mean Reciprocal Rank\n",
        "            results['hits@k']:      Probability of getting the correct preodiction in top-k ranks based on predicted score\n",
        "        \"\"\"\n",
        "        left_results = self.predict(split=split, mode='tail_batch')\n",
        "\n",
        "        right_results = self.predict(split=split, mode='head_batch')\n",
        "\n",
        "        results = get_combined_results(left_results, right_results)\n",
        "\n",
        "        res_mrr = '\\n\\tMRR: Tail : {:.5}, Head : {:.5}, Avg : {:.5}\\n'.format(results['left_mrr'],\n",
        "                                                                              results['right_mrr'],\n",
        "                                                                              results['mrr'])\n",
        "        res_mr = '\\tMR: Tail : {:.5}, Head : {:.5}, Avg : {:.5}\\n'.format(results['left_mr'],\n",
        "                                                                          results['right_mr'],\n",
        "                                                                          results['mr'])\n",
        "        res_hit1 = '\\tHit-1: Tail : {:.5}, Head : {:.5}, Avg : {:.5}\\n'.format(results['left_hits@1'],\n",
        "                                                                               results['right_hits@1'],\n",
        "                                                                               results['hits@1'])\n",
        "        res_hit3 = '\\tHit-3: Tail : {:.5}, Head : {:.5}, Avg : {:.5}\\n'.format(results['left_hits@3'],\n",
        "                                                                               results['right_hits@3'],\n",
        "                                                                               results['hits@3'])\n",
        "        res_hit10 = '\\tHit-10: Tail : {:.5}, Head : {:.5}, Avg : {:.5}'.format(results['left_hits@10'],\n",
        "                                                                               results['right_hits@10'],\n",
        "                                                                               results['hits@10'])\n",
        "        log_res = res_mrr + res_mr + res_hit1 + res_hit3 + res_hit10\n",
        "        if (epoch + 1) % 10 == 0 or split == 'test':\n",
        "            self.logger.info(\n",
        "                '[Evaluating Epoch {} {}]: {}'.format(epoch, split, log_res))\n",
        "        else:\n",
        "            self.logger.info(\n",
        "                '[Evaluating Epoch {} {}]: {}'.format(epoch, split, res_mrr))\n",
        "\n",
        "        return results\n",
        "\n",
        "    def predict(self, split='valid', mode='tail_batch'):\n",
        "        \"\"\"\n",
        "        Function to run model evaluation for a given mode\n",
        "        Parameters\n",
        "        ----------\n",
        "        split: (string) \tIf split == 'valid' then evaluate on the validation set, else the test set\n",
        "        mode: (string):\t\tCan be 'head_batch' or 'tail_batch'\n",
        "        Returns\n",
        "        -------\n",
        "        resutls:\t\t\tThe evaluation results containing the following:\n",
        "            results['mr']:         \tAverage of ranks_left and ranks_right\n",
        "            results['mrr']:         Mean Reciprocal Rank\n",
        "            results['hits@k']:      Probability of getting the correct preodiction in top-k ranks based on predicted score\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            results = {}\n",
        "            train_iter = iter(self.data_iter['{}_{}'.format(split, mode.split('_')[0])])\n",
        "\n",
        "            for step, batch in enumerate(train_iter):\n",
        "                sub, rel, obj, label = self.read_batch(batch, split)\n",
        "                pred = self.model.forward(sub, rel)\n",
        "                b_range = torch.arange(pred.size()[0], device=self.device)\n",
        "                target_pred = pred[b_range, obj]\n",
        "                # filter setting\n",
        "                pred = torch.where(label.byte(), -torch.ones_like(pred) * 10000000, pred)\n",
        "                pred[b_range, obj] = target_pred\n",
        "                ranks = 1 + torch.argsort(torch.argsort(pred, dim=1, descending=True), dim=1, descending=False)[\n",
        "                    b_range, obj]\n",
        "\n",
        "                ranks = ranks.float()\n",
        "                results['count'] = torch.numel(ranks) + results.get('count', 0.0)\n",
        "                results['mr'] = torch.sum(ranks).item() + results.get('mr', 0.0)\n",
        "                results['mrr'] = torch.sum(1.0 / ranks).item() + results.get('mrr', 0.0)\n",
        "                for k in range(10):\n",
        "                    results['hits@{}'.format(k + 1)] = torch.numel(ranks[ranks <= (k + 1)]) + results.get(\n",
        "                        'hits@{}'.format(k + 1), 0.0)\n",
        "\n",
        "                # if step % 100 == 0:\n",
        "                #     self.logger.info('[{}, {} Step {}]\\t{}'.format(split.title(), mode.title(), step, self.p.name))\n",
        "\n",
        "        return results\n",
        "\n",
        "    def run_epoch(self, epoch, val_mrr=0):\n",
        "        \"\"\"\n",
        "        Function to run one epoch of training\n",
        "        Parameters\n",
        "        ----------\n",
        "        epoch: current epoch count\n",
        "        Returns\n",
        "        -------\n",
        "        loss: The loss value after the completion of one epoch\n",
        "        \"\"\"\n",
        "        self.model.train()\n",
        "        losses = []\n",
        "        train_iter = iter(self.data_iter['train'])\n",
        "\n",
        "        for step, batch in enumerate(train_iter):\n",
        "            self.optimizer.zero_grad()\n",
        "            sub, rel, obj, label, neg_ent, sub_samp = self.read_batch(batch, 'train')\n",
        "\n",
        "            pred = self.model.forward(sub, rel, neg_ent)\n",
        "            loss = self.model.loss(pred, label)\n",
        "\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            # if step % 100 == 0:\n",
        "            #     self.logger.info('[E:{}| {}]: Train Loss:{:.5},  Val MRR:{:.5}\\t{}'.format(epoch, step, np.mean(losses),\n",
        "            #                                                                                self.best_val_mrr,\n",
        "            #                                                                                self.p.name))\n",
        "\n",
        "        loss = np.mean(losses)\n",
        "        # self.logger.info('[Epoch:{}]:  Training Loss:{:.4}\\n'.format(epoch, loss))\n",
        "        return loss\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"\n",
        "        Function to run training and evaluation of model\n",
        "        Parameters\n",
        "        ----------\n",
        "        Returns\n",
        "        -------\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.best_val_mrr, self.best_val, self.best_epoch, val_mrr = 0., {}, 0, 0.\n",
        "            save_path = os.path.join('./checkpoints', self.p.name)\n",
        "\n",
        "            if self.p.restore:\n",
        "                self.load_model(save_path)\n",
        "                self.logger.info('Successfully Loaded previous model')\n",
        "            val_results = {}\n",
        "            val_results['mrr'] = 0\n",
        "            for epoch in range(self.p.max_epochs):\n",
        "                train_loss = self.run_epoch(epoch, val_mrr)\n",
        "                # if ((epoch + 1) % 10 == 0):\n",
        "                val_results = self.evaluate('valid', epoch)\n",
        "\n",
        "                if val_results['mrr'] > self.best_val_mrr:\n",
        "                    self.best_val = val_results\n",
        "                    self.best_val_mrr = val_results['mrr']\n",
        "                    self.best_epoch = epoch\n",
        "                    self.save_model(save_path)\n",
        "\n",
        "                self.logger.info(\n",
        "                    '[Epoch {}]: Training Loss: {:.5}, Best valid MRR: {:.5}\\n\\n'.format(epoch, train_loss,\n",
        "                                                                                         self.best_val_mrr))\n",
        "\n",
        "            self.logger.info('Loading best model, Evaluating on Test data')\n",
        "            self.load_model(save_path)\n",
        "            test_results = self.evaluate('test', self.best_epoch)\n",
        "        except Exception as e:\n",
        "            self.logger.debug(\"%s____%s\\n\"\n",
        "                              \"traceback.format_exc():____%s\" % (Exception, e, traceback.format_exc()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSPhjl97Xuhh"
      },
      "outputs": [],
      "source": [
        "!mkdir checkpoints\n",
        "!mkdir log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtpyU9DaemzX"
      },
      "outputs": [],
      "source": [
        "log_dir = './log/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsr1sSDh_bBa"
      },
      "source": [
        "## FB15k-237"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qFkjgD53nmU"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTKD50vHzxn8"
      },
      "outputs": [],
      "source": [
        "class Hyperparameters():\n",
        "\n",
        "  def __init__(self):\n",
        "    self.name = 'testrun'          # Set run name for saving/restoring models (str)\n",
        "    self.name = self.name + '_' + time.strftime('%d_%m_%Y') + '_' + time.strftime('%H:%M:%S')\n",
        "    self.dataset = 'FB15k-237'     # Dataset to use (str)\n",
        "    self.model = 'ragat'           # Model name (str)\n",
        "    self.score_func = 'interacte'  # Score Function for Link prediction (str)\n",
        "    self.opn = 'cross'             # Composition Operation to be used in RAGAT (str)\n",
        "\n",
        "    self.batch_size = 1024         # Batch size (int)\n",
        "    self.test_batch_size = 1024    # Batch size of valid and test data (int)\n",
        "    self.gamma = 40.0              # Margin (float)\n",
        "    self.gpu = '0'                 # Set GPU Ids : Eg: For CPU = -1, For Single GPU = 0 (str)\n",
        "    self.max_epochs = 100          # Number of epochs (int)\n",
        "    self.l2 = 0.0                  # L2 Regularization for Optimizer (float)\n",
        "    self.lr = 0.001                # Starting Learning Rate (float)\n",
        "    self.lbl_smooth = 0.1          # Label Smoothing (float)\n",
        "    self.num_workers = 10          # Number of processes to construct batches (int)\n",
        "    self.seed = 41504              # Seed for randomization (int)\n",
        "\n",
        "    self.restore = False            # Restore from the previously saved model (True or False)\n",
        "    self.bias = True               # Whether to use bias in the model (True or False)\n",
        "\n",
        "    self.init_dim = 100            # Initial dimension size for entities and relations (int)\n",
        "    self.gcn_dim = 200             # Number of hidden units in GCN (int)\n",
        "    self.embed_dim = 200           # Embedding dimension to give as input to score function (int)\n",
        "    self.gcn_layer = 1             # Number of GCN Layers to use (int)\n",
        "    self.dropout = 0.4             # Dropout to use in GCN Layer (float)\n",
        "    self.hid_drop = 0.3            # Dropout after GCN\n",
        "\n",
        "    # ConvE specific hyperparameters\n",
        "    self.hid_drop2 = 0.3           # ConvE: Hidden dropout (float)\n",
        "    self.feat_drop = 0.3           # ConvE: Feature Dropout (float)\n",
        "    self.k_w = 10                  # ConvE: k_w (int)\n",
        "    self.k_h = 20                  # ConvE: k_h (int)\n",
        "    self.num_filt = 200            # ConvE: Number of filters in convolution (int)\n",
        "    self.ker_sz = 7                # ConvE: Kernel size to use (int)\n",
        "\n",
        "    self.log_dir = log_dir         # Log directory (str)\n",
        "    self.config_dir = './config/'  # Config directory (str)\n",
        "\n",
        "    # InteractE hyperparameters\n",
        "    self.neg_num = 1000            # Number of negative samples to use for loss calculation (int)\n",
        "    self.strategy = 'one_to_n'     # Training strategy to use (str)\n",
        "    self.form = 'plain'            # The reshaping form to use (str)\n",
        "    self.ik_w = 10                 # Width of the reshaped matrix (int)\n",
        "    self.ik_h = 20                 # Height of the reshaped matrix (int)\n",
        "    self.inum_filt = 200           # Number of filters in convolution (int)\n",
        "    self.iker_sz = 9               # Kernel size to use (int)\n",
        "    self.iperm = 1                 # Number of Feature rearrangement to use (int)\n",
        "    self.iinp_drop = 0.3           # Dropout for Input layer (float)\n",
        "    self.ifeat_drop = 0.4          # Dropout for Feature (float)\n",
        "    self.ihid_drop = 0.3           # Dropout for Hidden layer (float)\n",
        "    self.att = True                # Whether to use attention layer (True or False)\n",
        "    self.head_num = 2              # Number of attention head (int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMG3Ll9-8W1_"
      },
      "outputs": [],
      "source": [
        "args = Hyperparameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dh3OA9-G5Wmj"
      },
      "outputs": [],
      "source": [
        "np.set_printoptions(precision=4)\n",
        "set_gpu(args.gpu)\n",
        "np.random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed_all(args.seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkpwHrZz3sei"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qb9WDfvgCiPr"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzLnAFjC3mgc",
        "outputId": "a9071efd-029d-4b12-ca41-c145b37f8bd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-05-03 21:45:41,263 - [INFO] - {'name': 'testrun_03_05_2023_21:45:36', 'dataset': 'FB15k-237', 'model': 'ragat', 'score_func': 'interacte', 'opn': 'cross', 'batch_size': 1024, 'test_batch_size': 1024, 'gamma': 40.0, 'gpu': '0', 'max_epochs': 100, 'l2': 0.0, 'lr': 0.001, 'lbl_smooth': 0.1, 'num_workers': 10, 'seed': 41504, 'restore': False, 'bias': True, 'init_dim': 100, 'gcn_dim': 200, 'embed_dim': 200, 'gcn_layer': 1, 'dropout': 0.4, 'hid_drop': 0.3, 'hid_drop2': 0.3, 'feat_drop': 0.3, 'k_w': 10, 'k_h': 20, 'num_filt': 200, 'ker_sz': 7, 'log_dir': './log/', 'config_dir': './config/', 'neg_num': 1000, 'strategy': 'one_to_n', 'form': 'plain', 'ik_w': 10, 'ik_h': 20, 'inum_filt': 200, 'iker_sz': 9, 'iperm': 1, 'iinp_drop': 0.3, 'ifeat_drop': 0.4, 'ihid_drop': 0.3, 'att': True, 'head_num': 2}\n",
            "{'att': True,\n",
            " 'batch_size': 1024,\n",
            " 'bias': True,\n",
            " 'config_dir': './config/',\n",
            " 'dataset': 'FB15k-237',\n",
            " 'dropout': 0.4,\n",
            " 'embed_dim': 200,\n",
            " 'feat_drop': 0.3,\n",
            " 'form': 'plain',\n",
            " 'gamma': 40.0,\n",
            " 'gcn_dim': 200,\n",
            " 'gcn_layer': 1,\n",
            " 'gpu': '0',\n",
            " 'head_num': 2,\n",
            " 'hid_drop': 0.3,\n",
            " 'hid_drop2': 0.3,\n",
            " 'ifeat_drop': 0.4,\n",
            " 'ihid_drop': 0.3,\n",
            " 'iinp_drop': 0.3,\n",
            " 'ik_h': 20,\n",
            " 'ik_w': 10,\n",
            " 'iker_sz': 9,\n",
            " 'init_dim': 100,\n",
            " 'inum_filt': 200,\n",
            " 'iperm': 1,\n",
            " 'k_h': 20,\n",
            " 'k_w': 10,\n",
            " 'ker_sz': 7,\n",
            " 'l2': 0.0,\n",
            " 'lbl_smooth': 0.1,\n",
            " 'log_dir': './log/',\n",
            " 'lr': 0.001,\n",
            " 'max_epochs': 100,\n",
            " 'model': 'ragat',\n",
            " 'name': 'testrun_03_05_2023_21:45:36',\n",
            " 'neg_num': 1000,\n",
            " 'num_filt': 200,\n",
            " 'num_workers': 10,\n",
            " 'opn': 'cross',\n",
            " 'restore': False,\n",
            " 'score_func': 'interacte',\n",
            " 'seed': 41504,\n",
            " 'strategy': 'one_to_n',\n",
            " 'test_batch_size': 1024}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "<ipython-input-21-69d707ecd99e>:19: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  self.message_args = inspect.getargspec(self.message)[0][1:]\n",
            "<ipython-input-21-69d707ecd99e>:21: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  self.update_args = inspect.getargspec(self.update)[0][2:]\n",
            "<ipython-input-29-e695df900afa>:336: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:493.)\n",
            "  pred = torch.where(label.byte(), -torch.ones_like(pred) * 10000000, pred)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-05-03 21:47:09,398 - [INFO] - [Evaluating Epoch 0 valid]: \n",
            "\tMRR: Tail : 0.00032, Head : 0.00039, Avg : 0.00035\n",
            "\n",
            "2023-05-03 21:47:09,888 - [INFO] - [Epoch 0]: Training Loss: 0.084411, Best valid MRR: 0.00035\n",
            "\n",
            "\n",
            "2023-05-03 21:48:24,730 - [INFO] - [Evaluating Epoch 1 valid]: \n",
            "\tMRR: Tail : 0.0052, Head : 0.00052, Avg : 0.00286\n",
            "\n",
            "2023-05-03 21:48:25,458 - [INFO] - [Epoch 1]: Training Loss: 0.0064909, Best valid MRR: 0.00286\n",
            "\n",
            "\n",
            "2023-05-03 21:49:40,248 - [INFO] - [Evaluating Epoch 2 valid]: \n",
            "\tMRR: Tail : 0.01793, Head : 0.00424, Avg : 0.01109\n",
            "\n",
            "2023-05-03 21:49:41,036 - [INFO] - [Epoch 2]: Training Loss: 0.0040475, Best valid MRR: 0.01109\n",
            "\n",
            "\n",
            "2023-05-03 21:50:54,725 - [INFO] - [Evaluating Epoch 3 valid]: \n",
            "\tMRR: Tail : 0.04756, Head : 0.00279, Avg : 0.02517\n",
            "\n",
            "2023-05-03 21:50:55,414 - [INFO] - [Epoch 3]: Training Loss: 0.0033173, Best valid MRR: 0.02517\n",
            "\n",
            "\n",
            "2023-05-03 21:52:09,129 - [INFO] - [Evaluating Epoch 4 valid]: \n",
            "\tMRR: Tail : 0.09416, Head : 0.00721, Avg : 0.05069\n",
            "\n",
            "2023-05-03 21:52:09,769 - [INFO] - [Epoch 4]: Training Loss: 0.0030203, Best valid MRR: 0.05069\n",
            "\n",
            "\n",
            "2023-05-03 21:53:23,008 - [INFO] - [Evaluating Epoch 5 valid]: \n",
            "\tMRR: Tail : 0.12287, Head : 0.00859, Avg : 0.06573\n",
            "\n",
            "2023-05-03 21:53:23,530 - [INFO] - [Epoch 5]: Training Loss: 0.0027988, Best valid MRR: 0.06573\n",
            "\n",
            "\n",
            "2023-05-03 21:54:36,635 - [INFO] - [Evaluating Epoch 6 valid]: \n",
            "\tMRR: Tail : 0.15625, Head : 0.01275, Avg : 0.0845\n",
            "\n",
            "2023-05-03 21:54:37,148 - [INFO] - [Epoch 6]: Training Loss: 0.0026807, Best valid MRR: 0.0845\n",
            "\n",
            "\n",
            "2023-05-03 21:55:50,320 - [INFO] - [Evaluating Epoch 7 valid]: \n",
            "\tMRR: Tail : 0.19787, Head : 0.02222, Avg : 0.11005\n",
            "\n",
            "2023-05-03 21:55:50,824 - [INFO] - [Epoch 7]: Training Loss: 0.0025943, Best valid MRR: 0.11005\n",
            "\n",
            "\n",
            "2023-05-03 21:57:04,298 - [INFO] - [Evaluating Epoch 8 valid]: \n",
            "\tMRR: Tail : 0.21492, Head : 0.03304, Avg : 0.12398\n",
            "\n",
            "2023-05-03 21:57:04,814 - [INFO] - [Epoch 8]: Training Loss: 0.0025198, Best valid MRR: 0.12398\n",
            "\n",
            "\n",
            "2023-05-03 21:58:17,762 - [INFO] - [Evaluating Epoch 9 valid]: \n",
            "\tMRR: Tail : 0.2266, Head : 0.04319, Avg : 0.1349\n",
            "\tMR: Tail : 1730.4, Head : 2941.9, Avg : 2336.2\n",
            "\tHit-1: Tail : 0.17029, Head : 0.02253, Avg : 0.09641\n",
            "\tHit-3: Tail : 0.25218, Head : 0.04043, Avg : 0.14631\n",
            "\tHit-10: Tail : 0.3302, Head : 0.08839, Avg : 0.2093\n",
            "2023-05-03 21:58:18,265 - [INFO] - [Epoch 9]: Training Loss: 0.0024667, Best valid MRR: 0.1349\n",
            "\n",
            "\n",
            "2023-05-03 21:59:31,354 - [INFO] - [Evaluating Epoch 10 valid]: \n",
            "\tMRR: Tail : 0.25071, Head : 0.05534, Avg : 0.15302\n",
            "\n",
            "2023-05-03 21:59:31,882 - [INFO] - [Epoch 10]: Training Loss: 0.0024098, Best valid MRR: 0.15302\n",
            "\n",
            "\n",
            "2023-05-03 22:00:45,021 - [INFO] - [Evaluating Epoch 11 valid]: \n",
            "\tMRR: Tail : 0.25837, Head : 0.06458, Avg : 0.16148\n",
            "\n",
            "2023-05-03 22:00:45,523 - [INFO] - [Epoch 11]: Training Loss: 0.0023578, Best valid MRR: 0.16148\n",
            "\n",
            "\n",
            "2023-05-03 22:01:59,216 - [INFO] - [Evaluating Epoch 12 valid]: \n",
            "\tMRR: Tail : 0.26966, Head : 0.07529, Avg : 0.17248\n",
            "\n",
            "2023-05-03 22:01:59,742 - [INFO] - [Epoch 12]: Training Loss: 0.0023095, Best valid MRR: 0.17248\n",
            "\n",
            "\n",
            "2023-05-03 22:03:13,175 - [INFO] - [Evaluating Epoch 13 valid]: \n",
            "\tMRR: Tail : 0.27783, Head : 0.094, Avg : 0.18591\n",
            "\n",
            "2023-05-03 22:03:13,694 - [INFO] - [Epoch 13]: Training Loss: 0.0022709, Best valid MRR: 0.18591\n",
            "\n",
            "\n",
            "2023-05-03 22:04:27,120 - [INFO] - [Evaluating Epoch 14 valid]: \n",
            "\tMRR: Tail : 0.28953, Head : 0.09599, Avg : 0.19276\n",
            "\n",
            "2023-05-03 22:04:27,664 - [INFO] - [Epoch 14]: Training Loss: 0.0022284, Best valid MRR: 0.19276\n",
            "\n",
            "\n",
            "2023-05-03 22:05:40,643 - [INFO] - [Evaluating Epoch 15 valid]: \n",
            "\tMRR: Tail : 0.29889, Head : 0.10923, Avg : 0.20406\n",
            "\n",
            "2023-05-03 22:05:41,302 - [INFO] - [Epoch 15]: Training Loss: 0.0021886, Best valid MRR: 0.20406\n",
            "\n",
            "\n",
            "2023-05-03 22:06:54,692 - [INFO] - [Evaluating Epoch 16 valid]: \n",
            "\tMRR: Tail : 0.30268, Head : 0.0991, Avg : 0.20089\n",
            "\n",
            "2023-05-03 22:06:54,694 - [INFO] - [Epoch 16]: Training Loss: 0.0021699, Best valid MRR: 0.20406\n",
            "\n",
            "\n",
            "2023-05-03 22:08:07,844 - [INFO] - [Evaluating Epoch 17 valid]: \n",
            "\tMRR: Tail : 0.30566, Head : 0.11245, Avg : 0.20906\n",
            "\n",
            "2023-05-03 22:08:08,360 - [INFO] - [Epoch 17]: Training Loss: 0.0021443, Best valid MRR: 0.20906\n",
            "\n",
            "\n",
            "2023-05-03 22:09:21,819 - [INFO] - [Evaluating Epoch 18 valid]: \n",
            "\tMRR: Tail : 0.31257, Head : 0.11077, Avg : 0.21167\n",
            "\n",
            "2023-05-03 22:09:22,340 - [INFO] - [Epoch 18]: Training Loss: 0.002124, Best valid MRR: 0.21167\n",
            "\n",
            "\n",
            "2023-05-03 22:10:36,253 - [INFO] - [Evaluating Epoch 19 valid]: \n",
            "\tMRR: Tail : 0.3088, Head : 0.10927, Avg : 0.20903\n",
            "\tMR: Tail : 405.25, Head : 727.68, Avg : 566.46\n",
            "\tHit-1: Tail : 0.23656, Head : 0.06085, Avg : 0.1487\n",
            "\tHit-3: Tail : 0.33624, Head : 0.1164, Avg : 0.22632\n",
            "\tHit-10: Tail : 0.44939, Head : 0.202, Avg : 0.32569\n",
            "2023-05-03 22:10:36,254 - [INFO] - [Epoch 19]: Training Loss: 0.0020902, Best valid MRR: 0.21167\n",
            "\n",
            "\n",
            "2023-05-03 22:11:49,986 - [INFO] - [Evaluating Epoch 20 valid]: \n",
            "\tMRR: Tail : 0.30091, Head : 0.11209, Avg : 0.2065\n",
            "\n",
            "2023-05-03 22:11:49,988 - [INFO] - [Epoch 20]: Training Loss: 0.0020858, Best valid MRR: 0.21167\n",
            "\n",
            "\n",
            "2023-05-03 22:13:03,703 - [INFO] - [Evaluating Epoch 21 valid]: \n",
            "\tMRR: Tail : 0.30263, Head : 0.11191, Avg : 0.20727\n",
            "\n",
            "2023-05-03 22:13:03,705 - [INFO] - [Epoch 21]: Training Loss: 0.0020579, Best valid MRR: 0.21167\n",
            "\n",
            "\n",
            "2023-05-03 22:14:17,158 - [INFO] - [Evaluating Epoch 22 valid]: \n",
            "\tMRR: Tail : 0.3014, Head : 0.11549, Avg : 0.20845\n",
            "\n",
            "2023-05-03 22:14:17,160 - [INFO] - [Epoch 22]: Training Loss: 0.002059, Best valid MRR: 0.21167\n",
            "\n",
            "\n",
            "2023-05-03 22:15:30,413 - [INFO] - [Evaluating Epoch 23 valid]: \n",
            "\tMRR: Tail : 0.27184, Head : 0.11015, Avg : 0.191\n",
            "\n",
            "2023-05-03 22:15:30,415 - [INFO] - [Epoch 23]: Training Loss: 0.0020281, Best valid MRR: 0.21167\n",
            "\n",
            "\n",
            "2023-05-03 22:16:43,935 - [INFO] - [Evaluating Epoch 24 valid]: \n",
            "\tMRR: Tail : 0.27269, Head : 0.11637, Avg : 0.19453\n",
            "\n",
            "2023-05-03 22:16:43,937 - [INFO] - [Epoch 24]: Training Loss: 0.0020132, Best valid MRR: 0.21167\n",
            "\n",
            "\n",
            "2023-05-03 22:17:57,175 - [INFO] - [Evaluating Epoch 25 valid]: \n",
            "\tMRR: Tail : 0.29531, Head : 0.1167, Avg : 0.20601\n",
            "\n",
            "2023-05-03 22:17:57,177 - [INFO] - [Epoch 25]: Training Loss: 0.0019846, Best valid MRR: 0.21167\n",
            "\n",
            "\n",
            "2023-05-03 22:19:10,568 - [INFO] - [Evaluating Epoch 26 valid]: \n",
            "\tMRR: Tail : 0.32071, Head : 0.12561, Avg : 0.22316\n",
            "\n",
            "2023-05-03 22:19:11,102 - [INFO] - [Epoch 26]: Training Loss: 0.001959, Best valid MRR: 0.22316\n",
            "\n",
            "\n",
            "2023-05-03 22:20:24,436 - [INFO] - [Evaluating Epoch 27 valid]: \n",
            "\tMRR: Tail : 0.33428, Head : 0.12973, Avg : 0.232\n",
            "\n",
            "2023-05-03 22:20:24,968 - [INFO] - [Epoch 27]: Training Loss: 0.0019148, Best valid MRR: 0.232\n",
            "\n",
            "\n",
            "2023-05-03 22:21:38,205 - [INFO] - [Evaluating Epoch 28 valid]: \n",
            "\tMRR: Tail : 0.33705, Head : 0.13235, Avg : 0.2347\n",
            "\n",
            "2023-05-03 22:21:38,717 - [INFO] - [Epoch 28]: Training Loss: 0.0018933, Best valid MRR: 0.2347\n",
            "\n",
            "\n",
            "2023-05-03 22:22:52,369 - [INFO] - [Evaluating Epoch 29 valid]: \n",
            "\tMRR: Tail : 0.33688, Head : 0.13313, Avg : 0.23501\n",
            "\tMR: Tail : 330.03, Head : 576.05, Avg : 453.04\n",
            "\tHit-1: Tail : 0.26051, Head : 0.07841, Avg : 0.16946\n",
            "\tHit-3: Tail : 0.36806, Head : 0.14303, Avg : 0.25555\n",
            "\tHit-10: Tail : 0.48509, Head : 0.23872, Avg : 0.3619\n",
            "2023-05-03 22:22:52,912 - [INFO] - [Epoch 29]: Training Loss: 0.0018607, Best valid MRR: 0.23501\n",
            "\n",
            "\n",
            "2023-05-03 22:24:06,492 - [INFO] - [Evaluating Epoch 30 valid]: \n",
            "\tMRR: Tail : 0.33723, Head : 0.13759, Avg : 0.23741\n",
            "\n",
            "2023-05-03 22:24:07,016 - [INFO] - [Epoch 30]: Training Loss: 0.0018367, Best valid MRR: 0.23741\n",
            "\n",
            "\n",
            "2023-05-03 22:25:20,977 - [INFO] - [Evaluating Epoch 31 valid]: \n",
            "\tMRR: Tail : 0.34057, Head : 0.13706, Avg : 0.23882\n",
            "\n",
            "2023-05-03 22:25:21,506 - [INFO] - [Epoch 31]: Training Loss: 0.0018258, Best valid MRR: 0.23882\n",
            "\n",
            "\n",
            "2023-05-03 22:26:35,943 - [INFO] - [Evaluating Epoch 32 valid]: \n",
            "\tMRR: Tail : 0.34093, Head : 0.14173, Avg : 0.24133\n",
            "\n",
            "2023-05-03 22:26:36,477 - [INFO] - [Epoch 32]: Training Loss: 0.0018133, Best valid MRR: 0.24133\n",
            "\n",
            "\n",
            "2023-05-03 22:27:50,495 - [INFO] - [Evaluating Epoch 33 valid]: \n",
            "\tMRR: Tail : 0.3457, Head : 0.14496, Avg : 0.24533\n",
            "\n",
            "2023-05-03 22:27:51,001 - [INFO] - [Epoch 33]: Training Loss: 0.001793, Best valid MRR: 0.24533\n",
            "\n",
            "\n",
            "2023-05-03 22:29:04,991 - [INFO] - [Evaluating Epoch 34 valid]: \n",
            "\tMRR: Tail : 0.34945, Head : 0.14697, Avg : 0.24821\n",
            "\n",
            "2023-05-03 22:29:05,537 - [INFO] - [Epoch 34]: Training Loss: 0.0017867, Best valid MRR: 0.24821\n",
            "\n",
            "\n",
            "2023-05-03 22:30:19,102 - [INFO] - [Evaluating Epoch 35 valid]: \n",
            "\tMRR: Tail : 0.35368, Head : 0.15042, Avg : 0.25205\n",
            "\n",
            "2023-05-03 22:30:19,630 - [INFO] - [Epoch 35]: Training Loss: 0.0017768, Best valid MRR: 0.25205\n",
            "\n",
            "\n",
            "2023-05-03 22:31:32,958 - [INFO] - [Evaluating Epoch 36 valid]: \n",
            "\tMRR: Tail : 0.35609, Head : 0.15118, Avg : 0.25363\n",
            "\n",
            "2023-05-03 22:31:33,491 - [INFO] - [Epoch 36]: Training Loss: 0.0017624, Best valid MRR: 0.25363\n",
            "\n",
            "\n",
            "2023-05-03 22:32:46,917 - [INFO] - [Evaluating Epoch 37 valid]: \n",
            "\tMRR: Tail : 0.35662, Head : 0.15181, Avg : 0.25421\n",
            "\n",
            "2023-05-03 22:32:47,423 - [INFO] - [Epoch 37]: Training Loss: 0.0017538, Best valid MRR: 0.25421\n",
            "\n",
            "\n",
            "2023-05-03 22:34:01,088 - [INFO] - [Evaluating Epoch 38 valid]: \n",
            "\tMRR: Tail : 0.35974, Head : 0.15532, Avg : 0.25753\n",
            "\n",
            "2023-05-03 22:34:01,622 - [INFO] - [Epoch 38]: Training Loss: 0.0017457, Best valid MRR: 0.25753\n",
            "\n",
            "\n",
            "2023-05-03 22:35:15,456 - [INFO] - [Evaluating Epoch 39 valid]: \n",
            "\tMRR: Tail : 0.36165, Head : 0.1618, Avg : 0.26173\n",
            "\tMR: Tail : 197.77, Head : 435.57, Avg : 316.67\n",
            "\tHit-1: Tail : 0.27864, Head : 0.09997, Avg : 0.18931\n",
            "\tHit-3: Tail : 0.39532, Head : 0.17479, Avg : 0.28506\n",
            "\tHit-10: Tail : 0.52267, Head : 0.28383, Avg : 0.40325\n",
            "2023-05-03 22:35:15,979 - [INFO] - [Epoch 39]: Training Loss: 0.0017294, Best valid MRR: 0.26173\n",
            "\n",
            "\n",
            "2023-05-03 22:36:29,618 - [INFO] - [Evaluating Epoch 40 valid]: \n",
            "\tMRR: Tail : 0.36376, Head : 0.16938, Avg : 0.26657\n",
            "\n",
            "2023-05-03 22:36:30,153 - [INFO] - [Epoch 40]: Training Loss: 0.0017203, Best valid MRR: 0.26657\n",
            "\n",
            "\n",
            "2023-05-03 22:37:44,114 - [INFO] - [Evaluating Epoch 41 valid]: \n",
            "\tMRR: Tail : 0.3664, Head : 0.16901, Avg : 0.2677\n",
            "\n",
            "2023-05-03 22:37:44,629 - [INFO] - [Epoch 41]: Training Loss: 0.0017113, Best valid MRR: 0.2677\n",
            "\n",
            "\n",
            "2023-05-03 22:38:59,688 - [INFO] - [Evaluating Epoch 42 valid]: \n",
            "\tMRR: Tail : 0.36766, Head : 0.16861, Avg : 0.26814\n",
            "\n",
            "2023-05-03 22:39:00,245 - [INFO] - [Epoch 42]: Training Loss: 0.001701, Best valid MRR: 0.26814\n",
            "\n",
            "\n",
            "2023-05-03 22:40:15,297 - [INFO] - [Evaluating Epoch 43 valid]: \n",
            "\tMRR: Tail : 0.37125, Head : 0.1723, Avg : 0.27178\n",
            "\n",
            "2023-05-03 22:40:16,018 - [INFO] - [Epoch 43]: Training Loss: 0.0016847, Best valid MRR: 0.27178\n",
            "\n",
            "\n",
            "2023-05-03 22:41:29,715 - [INFO] - [Evaluating Epoch 44 valid]: \n",
            "\tMRR: Tail : 0.37414, Head : 0.17653, Avg : 0.27534\n",
            "\n",
            "2023-05-03 22:41:30,253 - [INFO] - [Epoch 44]: Training Loss: 0.0016778, Best valid MRR: 0.27534\n",
            "\n",
            "\n",
            "2023-05-03 22:42:43,430 - [INFO] - [Evaluating Epoch 45 valid]: \n",
            "\tMRR: Tail : 0.3738, Head : 0.17629, Avg : 0.27504\n",
            "\n",
            "2023-05-03 22:42:43,432 - [INFO] - [Epoch 45]: Training Loss: 0.0016749, Best valid MRR: 0.27534\n",
            "\n",
            "\n",
            "2023-05-03 22:43:57,008 - [INFO] - [Evaluating Epoch 46 valid]: \n",
            "\tMRR: Tail : 0.37883, Head : 0.17638, Avg : 0.2776\n",
            "\n",
            "2023-05-03 22:43:57,551 - [INFO] - [Epoch 46]: Training Loss: 0.0016618, Best valid MRR: 0.2776\n",
            "\n",
            "\n",
            "2023-05-03 22:45:10,733 - [INFO] - [Evaluating Epoch 47 valid]: \n",
            "\tMRR: Tail : 0.38426, Head : 0.17895, Avg : 0.2816\n",
            "\n",
            "2023-05-03 22:45:11,289 - [INFO] - [Epoch 47]: Training Loss: 0.0016573, Best valid MRR: 0.2816\n",
            "\n",
            "\n",
            "2023-05-03 22:46:24,494 - [INFO] - [Evaluating Epoch 48 valid]: \n",
            "\tMRR: Tail : 0.38551, Head : 0.18435, Avg : 0.28493\n",
            "\n",
            "2023-05-03 22:46:25,031 - [INFO] - [Epoch 48]: Training Loss: 0.0016418, Best valid MRR: 0.28493\n",
            "\n",
            "\n",
            "2023-05-03 22:47:38,482 - [INFO] - [Evaluating Epoch 49 valid]: \n",
            "\tMRR: Tail : 0.38454, Head : 0.18215, Avg : 0.28334\n",
            "\tMR: Tail : 154.57, Head : 396.17, Avg : 275.37\n",
            "\tHit-1: Tail : 0.29752, Head : 0.11315, Avg : 0.20533\n",
            "\tHit-3: Tail : 0.42059, Head : 0.19852, Avg : 0.30955\n",
            "\tHit-10: Tail : 0.55386, Head : 0.31531, Avg : 0.43459\n",
            "2023-05-03 22:47:38,484 - [INFO] - [Epoch 49]: Training Loss: 0.0016362, Best valid MRR: 0.28493\n",
            "\n",
            "\n",
            "2023-05-03 22:48:51,542 - [INFO] - [Evaluating Epoch 50 valid]: \n",
            "\tMRR: Tail : 0.38785, Head : 0.18753, Avg : 0.28769\n",
            "\n",
            "2023-05-03 22:48:52,083 - [INFO] - [Epoch 50]: Training Loss: 0.0016238, Best valid MRR: 0.28769\n",
            "\n",
            "\n",
            "2023-05-03 22:50:05,859 - [INFO] - [Evaluating Epoch 51 valid]: \n",
            "\tMRR: Tail : 0.39031, Head : 0.18432, Avg : 0.28732\n",
            "\n",
            "2023-05-03 22:50:05,862 - [INFO] - [Epoch 51]: Training Loss: 0.001629, Best valid MRR: 0.28769\n",
            "\n",
            "\n",
            "2023-05-03 22:51:19,389 - [INFO] - [Evaluating Epoch 52 valid]: \n",
            "\tMRR: Tail : 0.39305, Head : 0.19283, Avg : 0.29294\n",
            "\n",
            "2023-05-03 22:51:19,914 - [INFO] - [Epoch 52]: Training Loss: 0.0016151, Best valid MRR: 0.29294\n",
            "\n",
            "\n",
            "2023-05-03 22:52:34,037 - [INFO] - [Evaluating Epoch 53 valid]: \n",
            "\tMRR: Tail : 0.39384, Head : 0.19343, Avg : 0.29363\n",
            "\n",
            "2023-05-03 22:52:34,568 - [INFO] - [Epoch 53]: Training Loss: 0.0016085, Best valid MRR: 0.29363\n",
            "\n",
            "\n",
            "2023-05-03 22:53:49,088 - [INFO] - [Evaluating Epoch 54 valid]: \n",
            "\tMRR: Tail : 0.39756, Head : 0.19448, Avg : 0.29602\n",
            "\n",
            "2023-05-03 22:53:49,595 - [INFO] - [Epoch 54]: Training Loss: 0.0016055, Best valid MRR: 0.29602\n",
            "\n",
            "\n",
            "2023-05-03 22:55:03,923 - [INFO] - [Evaluating Epoch 55 valid]: \n",
            "\tMRR: Tail : 0.39985, Head : 0.19704, Avg : 0.29844\n",
            "\n",
            "2023-05-03 22:55:04,465 - [INFO] - [Epoch 55]: Training Loss: 0.0015946, Best valid MRR: 0.29844\n",
            "\n",
            "\n",
            "2023-05-03 22:56:18,492 - [INFO] - [Evaluating Epoch 56 valid]: \n",
            "\tMRR: Tail : 0.4008, Head : 0.1963, Avg : 0.29855\n",
            "\n",
            "2023-05-03 22:56:19,005 - [INFO] - [Epoch 56]: Training Loss: 0.0015858, Best valid MRR: 0.29855\n",
            "\n",
            "\n",
            "2023-05-03 22:57:32,580 - [INFO] - [Evaluating Epoch 57 valid]: \n",
            "\tMRR: Tail : 0.40145, Head : 0.19474, Avg : 0.29809\n",
            "\n",
            "2023-05-03 22:57:32,581 - [INFO] - [Epoch 57]: Training Loss: 0.0015876, Best valid MRR: 0.29855\n",
            "\n",
            "\n",
            "2023-05-03 22:58:46,308 - [INFO] - [Evaluating Epoch 58 valid]: \n",
            "\tMRR: Tail : 0.40384, Head : 0.20007, Avg : 0.30196\n",
            "\n",
            "2023-05-03 22:58:46,842 - [INFO] - [Epoch 58]: Training Loss: 0.0015862, Best valid MRR: 0.30196\n",
            "\n",
            "\n",
            "2023-05-03 23:00:00,310 - [INFO] - [Evaluating Epoch 59 valid]: \n",
            "\tMRR: Tail : 0.40633, Head : 0.20421, Avg : 0.30527\n",
            "\tMR: Tail : 143.1, Head : 352.36, Avg : 247.73\n",
            "\tHit-1: Tail : 0.31868, Head : 0.13134, Avg : 0.22501\n",
            "\tHit-3: Tail : 0.44169, Head : 0.22036, Avg : 0.33102\n",
            "\tHit-10: Tail : 0.58181, Head : 0.34953, Avg : 0.46567\n",
            "2023-05-03 23:00:00,834 - [INFO] - [Epoch 59]: Training Loss: 0.0015793, Best valid MRR: 0.30527\n",
            "\n",
            "\n",
            "2023-05-03 23:01:14,663 - [INFO] - [Evaluating Epoch 60 valid]: \n",
            "\tMRR: Tail : 0.40929, Head : 0.20221, Avg : 0.30575\n",
            "\n",
            "2023-05-03 23:01:15,212 - [INFO] - [Epoch 60]: Training Loss: 0.001574, Best valid MRR: 0.30575\n",
            "\n",
            "\n",
            "2023-05-03 23:02:28,912 - [INFO] - [Evaluating Epoch 61 valid]: \n",
            "\tMRR: Tail : 0.41038, Head : 0.20588, Avg : 0.30813\n",
            "\n",
            "2023-05-03 23:02:29,466 - [INFO] - [Epoch 61]: Training Loss: 0.0015671, Best valid MRR: 0.30813\n",
            "\n",
            "\n",
            "2023-05-03 23:03:43,343 - [INFO] - [Evaluating Epoch 62 valid]: \n",
            "\tMRR: Tail : 0.41049, Head : 0.20332, Avg : 0.3069\n",
            "\n",
            "2023-05-03 23:03:43,345 - [INFO] - [Epoch 62]: Training Loss: 0.0015635, Best valid MRR: 0.30813\n",
            "\n",
            "\n",
            "2023-05-03 23:04:56,997 - [INFO] - [Evaluating Epoch 63 valid]: \n",
            "\tMRR: Tail : 0.41157, Head : 0.2067, Avg : 0.30914\n",
            "\n",
            "2023-05-03 23:04:57,535 - [INFO] - [Epoch 63]: Training Loss: 0.0015615, Best valid MRR: 0.30914\n",
            "\n",
            "\n",
            "2023-05-03 23:06:11,405 - [INFO] - [Evaluating Epoch 64 valid]: \n",
            "\tMRR: Tail : 0.41606, Head : 0.2053, Avg : 0.31068\n",
            "\n",
            "2023-05-03 23:06:11,916 - [INFO] - [Epoch 64]: Training Loss: 0.0015527, Best valid MRR: 0.31068\n",
            "\n",
            "\n",
            "2023-05-03 23:07:26,421 - [INFO] - [Evaluating Epoch 65 valid]: \n",
            "\tMRR: Tail : 0.41147, Head : 0.20583, Avg : 0.30865\n",
            "\n",
            "2023-05-03 23:07:26,424 - [INFO] - [Epoch 65]: Training Loss: 0.0015495, Best valid MRR: 0.31068\n",
            "\n",
            "\n",
            "2023-05-03 23:08:41,027 - [INFO] - [Evaluating Epoch 66 valid]: \n",
            "\tMRR: Tail : 0.41678, Head : 0.20981, Avg : 0.3133\n",
            "\n",
            "2023-05-03 23:08:41,569 - [INFO] - [Epoch 66]: Training Loss: 0.0015476, Best valid MRR: 0.3133\n",
            "\n",
            "\n",
            "2023-05-03 23:09:56,512 - [INFO] - [Evaluating Epoch 67 valid]: \n",
            "\tMRR: Tail : 0.41871, Head : 0.21025, Avg : 0.31448\n",
            "\n",
            "2023-05-03 23:09:57,128 - [INFO] - [Epoch 67]: Training Loss: 0.0015456, Best valid MRR: 0.31448\n",
            "\n",
            "\n",
            "2023-05-03 23:11:11,609 - [INFO] - [Evaluating Epoch 68 valid]: \n",
            "\tMRR: Tail : 0.41467, Head : 0.21097, Avg : 0.31282\n",
            "\n",
            "2023-05-03 23:11:11,613 - [INFO] - [Epoch 68]: Training Loss: 0.0015435, Best valid MRR: 0.31448\n",
            "\n",
            "\n",
            "2023-05-03 23:12:25,493 - [INFO] - [Evaluating Epoch 69 valid]: \n",
            "\tMRR: Tail : 0.4208, Head : 0.20969, Avg : 0.31525\n",
            "\tMR: Tail : 131.84, Head : 329.21, Avg : 230.52\n",
            "\tHit-1: Tail : 0.33259, Head : 0.13368, Avg : 0.23313\n",
            "\tHit-3: Tail : 0.456, Head : 0.22578, Avg : 0.34089\n",
            "\tHit-10: Tail : 0.59681, Head : 0.36641, Avg : 0.48161\n",
            "2023-05-03 23:12:26,106 - [INFO] - [Epoch 69]: Training Loss: 0.0015382, Best valid MRR: 0.31525\n",
            "\n",
            "\n",
            "2023-05-03 23:13:39,657 - [INFO] - [Evaluating Epoch 70 valid]: \n",
            "\tMRR: Tail : 0.42127, Head : 0.20959, Avg : 0.31543\n",
            "\n",
            "2023-05-03 23:13:40,229 - [INFO] - [Epoch 70]: Training Loss: 0.0015369, Best valid MRR: 0.31543\n",
            "\n",
            "\n",
            "2023-05-03 23:14:53,747 - [INFO] - [Evaluating Epoch 71 valid]: \n",
            "\tMRR: Tail : 0.42105, Head : 0.21126, Avg : 0.31615\n",
            "\n",
            "2023-05-03 23:14:54,269 - [INFO] - [Epoch 71]: Training Loss: 0.0015378, Best valid MRR: 0.31615\n",
            "\n",
            "\n",
            "2023-05-03 23:16:07,514 - [INFO] - [Evaluating Epoch 72 valid]: \n",
            "\tMRR: Tail : 0.42106, Head : 0.2094, Avg : 0.31523\n",
            "\n",
            "2023-05-03 23:16:07,516 - [INFO] - [Epoch 72]: Training Loss: 0.0015395, Best valid MRR: 0.31615\n",
            "\n",
            "\n",
            "2023-05-03 23:17:20,605 - [INFO] - [Evaluating Epoch 73 valid]: \n",
            "\tMRR: Tail : 0.42392, Head : 0.20972, Avg : 0.31682\n",
            "\n",
            "2023-05-03 23:17:21,124 - [INFO] - [Epoch 73]: Training Loss: 0.0015393, Best valid MRR: 0.31682\n",
            "\n",
            "\n",
            "2023-05-03 23:18:34,904 - [INFO] - [Evaluating Epoch 74 valid]: \n",
            "\tMRR: Tail : 0.42441, Head : 0.21359, Avg : 0.319\n",
            "\n",
            "2023-05-03 23:18:35,453 - [INFO] - [Epoch 74]: Training Loss: 0.0015295, Best valid MRR: 0.319\n",
            "\n",
            "\n",
            "2023-05-03 23:19:49,027 - [INFO] - [Evaluating Epoch 75 valid]: \n",
            "\tMRR: Tail : 0.42442, Head : 0.21403, Avg : 0.31923\n",
            "\n",
            "2023-05-03 23:19:49,560 - [INFO] - [Epoch 75]: Training Loss: 0.0015318, Best valid MRR: 0.31923\n",
            "\n",
            "\n",
            "2023-05-03 23:21:03,110 - [INFO] - [Evaluating Epoch 76 valid]: \n",
            "\tMRR: Tail : 0.42654, Head : 0.21181, Avg : 0.31918\n",
            "\n",
            "2023-05-03 23:21:03,111 - [INFO] - [Epoch 76]: Training Loss: 0.0015234, Best valid MRR: 0.31923\n",
            "\n",
            "\n",
            "2023-05-03 23:22:16,930 - [INFO] - [Evaluating Epoch 77 valid]: \n",
            "\tMRR: Tail : 0.42693, Head : 0.21609, Avg : 0.32151\n",
            "\n",
            "2023-05-03 23:22:17,470 - [INFO] - [Epoch 77]: Training Loss: 0.0015211, Best valid MRR: 0.32151\n",
            "\n",
            "\n",
            "2023-05-03 23:23:31,681 - [INFO] - [Evaluating Epoch 78 valid]: \n",
            "\tMRR: Tail : 0.42418, Head : 0.22041, Avg : 0.32229\n",
            "\n",
            "2023-05-03 23:23:32,214 - [INFO] - [Epoch 78]: Training Loss: 0.0015136, Best valid MRR: 0.32229\n",
            "\n",
            "\n",
            "2023-05-03 23:24:46,847 - [INFO] - [Evaluating Epoch 79 valid]: \n",
            "\tMRR: Tail : 0.42686, Head : 0.2187, Avg : 0.32278\n",
            "\tMR: Tail : 125.63, Head : 318.72, Avg : 222.17\n",
            "\tHit-1: Tail : 0.33767, Head : 0.14018, Avg : 0.23892\n",
            "\tHit-3: Tail : 0.46353, Head : 0.23536, Avg : 0.34944\n",
            "\tHit-10: Tail : 0.60479, Head : 0.38021, Avg : 0.4925\n",
            "2023-05-03 23:24:47,407 - [INFO] - [Epoch 79]: Training Loss: 0.001518, Best valid MRR: 0.32278\n",
            "\n",
            "\n",
            "2023-05-03 23:26:02,210 - [INFO] - [Evaluating Epoch 80 valid]: \n",
            "\tMRR: Tail : 0.42821, Head : 0.22229, Avg : 0.32525\n",
            "\n",
            "2023-05-03 23:26:02,733 - [INFO] - [Epoch 80]: Training Loss: 0.001512, Best valid MRR: 0.32525\n",
            "\n",
            "\n",
            "2023-05-03 23:27:16,643 - [INFO] - [Evaluating Epoch 81 valid]: \n",
            "\tMRR: Tail : 0.42766, Head : 0.22108, Avg : 0.32437\n",
            "\n",
            "2023-05-03 23:27:16,645 - [INFO] - [Epoch 81]: Training Loss: 0.0015102, Best valid MRR: 0.32525\n",
            "\n",
            "\n",
            "2023-05-03 23:28:30,349 - [INFO] - [Evaluating Epoch 82 valid]: \n",
            "\tMRR: Tail : 0.43037, Head : 0.21263, Avg : 0.3215\n",
            "\n",
            "2023-05-03 23:28:30,351 - [INFO] - [Epoch 82]: Training Loss: 0.0015078, Best valid MRR: 0.32525\n",
            "\n",
            "\n",
            "2023-05-03 23:29:44,155 - [INFO] - [Evaluating Epoch 83 valid]: \n",
            "\tMRR: Tail : 0.42906, Head : 0.21957, Avg : 0.32432\n",
            "\n",
            "2023-05-03 23:29:44,157 - [INFO] - [Epoch 83]: Training Loss: 0.0015048, Best valid MRR: 0.32525\n",
            "\n",
            "\n",
            "2023-05-03 23:30:58,114 - [INFO] - [Evaluating Epoch 84 valid]: \n",
            "\tMRR: Tail : 0.42875, Head : 0.21685, Avg : 0.3228\n",
            "\n",
            "2023-05-03 23:30:58,116 - [INFO] - [Epoch 84]: Training Loss: 0.0015024, Best valid MRR: 0.32525\n",
            "\n",
            "\n",
            "2023-05-03 23:32:11,806 - [INFO] - [Evaluating Epoch 85 valid]: \n",
            "\tMRR: Tail : 0.42988, Head : 0.22025, Avg : 0.32507\n",
            "\n",
            "2023-05-03 23:32:11,808 - [INFO] - [Epoch 85]: Training Loss: 0.0015042, Best valid MRR: 0.32525\n",
            "\n",
            "\n",
            "2023-05-03 23:33:25,202 - [INFO] - [Evaluating Epoch 86 valid]: \n",
            "\tMRR: Tail : 0.43026, Head : 0.22015, Avg : 0.3252\n",
            "\n",
            "2023-05-03 23:33:25,203 - [INFO] - [Epoch 86]: Training Loss: 0.0015033, Best valid MRR: 0.32525\n",
            "\n",
            "\n",
            "2023-05-03 23:34:38,709 - [INFO] - [Evaluating Epoch 87 valid]: \n",
            "\tMRR: Tail : 0.43289, Head : 0.21968, Avg : 0.32628\n",
            "\n",
            "2023-05-03 23:34:39,226 - [INFO] - [Epoch 87]: Training Loss: 0.001501, Best valid MRR: 0.32628\n",
            "\n",
            "\n",
            "2023-05-03 23:35:52,732 - [INFO] - [Evaluating Epoch 88 valid]: \n",
            "\tMRR: Tail : 0.43368, Head : 0.22273, Avg : 0.3282\n",
            "\n",
            "2023-05-03 23:35:53,282 - [INFO] - [Epoch 88]: Training Loss: 0.0014975, Best valid MRR: 0.3282\n",
            "\n",
            "\n",
            "2023-05-03 23:37:07,290 - [INFO] - [Evaluating Epoch 89 valid]: \n",
            "\tMRR: Tail : 0.43347, Head : 0.22222, Avg : 0.32785\n",
            "\tMR: Tail : 118.41, Head : 305.57, Avg : 211.99\n",
            "\tHit-1: Tail : 0.34269, Head : 0.14326, Avg : 0.24297\n",
            "\tHit-3: Tail : 0.47248, Head : 0.24157, Avg : 0.35703\n",
            "\tHit-10: Tail : 0.61386, Head : 0.38335, Avg : 0.4986\n",
            "2023-05-03 23:37:07,292 - [INFO] - [Epoch 89]: Training Loss: 0.0014975, Best valid MRR: 0.3282\n",
            "\n",
            "\n",
            "2023-05-03 23:38:20,891 - [INFO] - [Evaluating Epoch 90 valid]: \n",
            "\tMRR: Tail : 0.43199, Head : 0.22413, Avg : 0.32806\n",
            "\n",
            "2023-05-03 23:38:20,899 - [INFO] - [Epoch 90]: Training Loss: 0.0014945, Best valid MRR: 0.3282\n",
            "\n",
            "\n",
            "2023-05-03 23:39:34,650 - [INFO] - [Evaluating Epoch 91 valid]: \n",
            "\tMRR: Tail : 0.43192, Head : 0.22463, Avg : 0.32827\n",
            "\n",
            "2023-05-03 23:39:35,187 - [INFO] - [Epoch 91]: Training Loss: 0.0014984, Best valid MRR: 0.32827\n",
            "\n",
            "\n",
            "2023-05-03 23:40:48,843 - [INFO] - [Evaluating Epoch 92 valid]: \n",
            "\tMRR: Tail : 0.43143, Head : 0.22326, Avg : 0.32734\n",
            "\n",
            "2023-05-03 23:40:48,844 - [INFO] - [Epoch 92]: Training Loss: 0.0014969, Best valid MRR: 0.32827\n",
            "\n",
            "\n",
            "2023-05-03 23:42:02,655 - [INFO] - [Evaluating Epoch 93 valid]: \n",
            "\tMRR: Tail : 0.43291, Head : 0.22787, Avg : 0.33039\n",
            "\n",
            "2023-05-03 23:42:03,194 - [INFO] - [Epoch 93]: Training Loss: 0.0014922, Best valid MRR: 0.33039\n",
            "\n",
            "\n",
            "2023-05-03 23:43:17,409 - [INFO] - [Evaluating Epoch 94 valid]: \n",
            "\tMRR: Tail : 0.43516, Head : 0.22693, Avg : 0.33104\n",
            "\n",
            "2023-05-03 23:43:17,932 - [INFO] - [Epoch 94]: Training Loss: 0.0014892, Best valid MRR: 0.33104\n",
            "\n",
            "\n",
            "2023-05-03 23:44:32,707 - [INFO] - [Evaluating Epoch 95 valid]: \n",
            "\tMRR: Tail : 0.43419, Head : 0.22672, Avg : 0.33046\n",
            "\n",
            "2023-05-03 23:44:32,714 - [INFO] - [Epoch 95]: Training Loss: 0.0014883, Best valid MRR: 0.33104\n",
            "\n",
            "\n",
            "2023-05-03 23:45:47,875 - [INFO] - [Evaluating Epoch 96 valid]: \n",
            "\tMRR: Tail : 0.43499, Head : 0.22511, Avg : 0.33005\n",
            "\n",
            "2023-05-03 23:45:47,876 - [INFO] - [Epoch 96]: Training Loss: 0.0014868, Best valid MRR: 0.33104\n",
            "\n",
            "\n",
            "2023-05-03 23:47:03,258 - [INFO] - [Evaluating Epoch 97 valid]: \n",
            "\tMRR: Tail : 0.43413, Head : 0.22703, Avg : 0.33058\n",
            "\n",
            "2023-05-03 23:47:03,260 - [INFO] - [Epoch 97]: Training Loss: 0.0014839, Best valid MRR: 0.33104\n",
            "\n",
            "\n",
            "2023-05-03 23:48:17,591 - [INFO] - [Evaluating Epoch 98 valid]: \n",
            "\tMRR: Tail : 0.43511, Head : 0.22719, Avg : 0.33115\n",
            "\n",
            "2023-05-03 23:48:18,207 - [INFO] - [Epoch 98]: Training Loss: 0.0014825, Best valid MRR: 0.33115\n",
            "\n",
            "\n",
            "2023-05-03 23:49:32,043 - [INFO] - [Evaluating Epoch 99 valid]: \n",
            "\tMRR: Tail : 0.43661, Head : 0.22929, Avg : 0.33295\n",
            "\tMR: Tail : 113.93, Head : 293.05, Avg : 203.49\n",
            "\tHit-1: Tail : 0.34497, Head : 0.14913, Avg : 0.24705\n",
            "\tHit-3: Tail : 0.47568, Head : 0.24973, Avg : 0.3627\n",
            "\tHit-10: Tail : 0.61967, Head : 0.39219, Avg : 0.50593\n",
            "2023-05-03 23:49:32,581 - [INFO] - [Epoch 99]: Training Loss: 0.0014831, Best valid MRR: 0.33295\n",
            "\n",
            "\n",
            "2023-05-03 23:49:32,584 - [INFO] - Loading best model, Evaluating on Test data\n",
            "2023-05-03 23:49:46,666 - [INFO] - [Evaluating Epoch 99 test]: \n",
            "\tMRR: Tail : 0.43324, Head : 0.22828, Avg : 0.33076\n",
            "\tMR: Tail : 121.53, Head : 299.17, Avg : 210.35\n",
            "\tHit-1: Tail : 0.34066, Head : 0.1481, Avg : 0.24438\n",
            "\tHit-3: Tail : 0.47361, Head : 0.2467, Avg : 0.36016\n",
            "\tHit-10: Tail : 0.6157, Head : 0.39202, Avg : 0.50386\n"
          ]
        }
      ],
      "source": [
        "model = Runner(args)\n",
        "model.fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbLreeq5_gSa"
      },
      "source": [
        "## WN18RR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uG3FqUe_itP"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKngiP5WIWPc"
      },
      "outputs": [],
      "source": [
        "class Hyperparameters():\n",
        "\n",
        "  def __init__(self):\n",
        "    self.name = 'testrun'          # Set run name for saving/restoring models (str)\n",
        "    self.name = self.name + '_' + time.strftime('%d_%m_%Y') + '_' + time.strftime('%H:%M:%S')\n",
        "    self.dataset = 'WN18RR'     # Dataset to use (str)\n",
        "    self.model = 'ragat'           # Model name (str)\n",
        "    self.score_func = 'interacte'  # Score Function for Link prediction (str)\n",
        "    self.opn = 'cross'             # Composition Operation to be used in RAGAT (str)\n",
        "\n",
        "    self.batch_size = 256          # Batch size (int)\n",
        "    self.test_batch_size = 256     # Batch size of valid and test data (int)\n",
        "    self.gamma = 40.0              # Margin (float)\n",
        "    self.gpu = '0'                 # Set GPU Ids : Eg: For CPU = -1, For Single GPU = 0 (str)\n",
        "    self.max_epochs = 200          # Number of epochs (int)\n",
        "    self.l2 = 0.0                  # L2 Regularization for Optimizer (float)\n",
        "    self.lr = 0.001                # Starting Learning Rate (float)\n",
        "    self.lbl_smooth = 0.1          # Label Smoothing (float)\n",
        "    self.num_workers = 10          # Number of processes to construct batches (int)\n",
        "    self.seed = 41504              # Seed for randomization (int)\n",
        "\n",
        "    self.restore = False            # Restore from the previously saved model (True or False)\n",
        "    self.bias = True               # Whether to use bias in the model (True or False)\n",
        "\n",
        "    self.init_dim = 100            # Initial dimension size for entities and relations (int)\n",
        "    self.gcn_dim = 200             # Number of hidden units in GCN (int)\n",
        "    self.embed_dim = 200           # Embedding dimension to give as input to score function (int)\n",
        "    self.gcn_layer = 1             # Number of GCN Layers to use (int)\n",
        "    self.dropout = 0.4             # Dropout to use in GCN Layer (float)\n",
        "    self.hid_drop = 0.3            # Dropout after GCN\n",
        "\n",
        "    # ConvE specific hyperparameters\n",
        "    self.hid_drop2 = 0.3           # ConvE: Hidden dropout (float)\n",
        "    self.feat_drop = 0.3           # ConvE: Feature Dropout (float)\n",
        "    self.k_w = 10                  # ConvE: k_w (int)\n",
        "    self.k_h = 20                  # ConvE: k_h (int)\n",
        "    self.num_filt = 200            # ConvE: Number of filters in convolution (int)\n",
        "    self.ker_sz = 7                # ConvE: Kernel size to use (int)\n",
        "\n",
        "    self.log_dir = log_dir         # Log directory (str)\n",
        "    self.config_dir = './config/'  # Config directory (str)\n",
        "\n",
        "    # InteractE hyperparameters\n",
        "    self.neg_num = 1000            # Number of negative samples to use for loss calculation (int)\n",
        "    self.strategy = 'one_to_n'     # Training strategy to use (str)\n",
        "    self.form = 'plain'            # The reshaping form to use (str)\n",
        "    self.ik_w = 10                 # Width of the reshaped matrix (int)\n",
        "    self.ik_h = 20                 # Height of the reshaped matrix (int)\n",
        "    self.inum_filt = 200           # Number of filters in convolution (int)\n",
        "    self.iker_sz = 11              # Kernel size to use (int)\n",
        "    self.iperm = 4                 # Number of Feature rearrangement to use (int)\n",
        "    self.iinp_drop = 0.3           # Dropout for Input layer (float)\n",
        "    self.ifeat_drop = 0.2          # Dropout for Feature (float)\n",
        "    self.ihid_drop = 0.3           # Dropout for Hidden layer (float)\n",
        "    self.att = True                # Whether to use attention layer (True or False)\n",
        "    self.head_num = 2              # Number of attention head (int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4P780Ra_6Xa"
      },
      "outputs": [],
      "source": [
        "args = Hyperparameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlEYpXt-_8On"
      },
      "outputs": [],
      "source": [
        "np.set_printoptions(precision=4)\n",
        "set_gpu(args.gpu)\n",
        "np.random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed_all(args.seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3fOpQ1D_8dU"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uadAds4v_9JS"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWI7ZOODAB_n",
        "outputId": "83f176f1-d294-4264-f944-01fa1bc765a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-10-26 08:27:57,975 - [INFO] - {'name': 'testrun_26_10_2022_08:27:57', 'dataset': 'WN18RR', 'model': 'ragat', 'score_func': 'interacte', 'opn': 'cross', 'batch_size': 256, 'test_batch_size': 256, 'gamma': 40.0, 'gpu': '0', 'max_epochs': 200, 'l2': 0.0, 'lr': 0.001, 'lbl_smooth': 0.1, 'num_workers': 10, 'seed': 41504, 'restore': False, 'bias': True, 'init_dim': 100, 'gcn_dim': 200, 'embed_dim': 200, 'gcn_layer': 1, 'dropout': 0.4, 'hid_drop': 0.3, 'hid_drop2': 0.3, 'feat_drop': 0.3, 'k_w': 10, 'k_h': 20, 'num_filt': 200, 'ker_sz': 7, 'log_dir': '/content/drive/MyDrive/RAGAT/log/', 'config_dir': './config/', 'neg_num': 1000, 'strategy': 'one_to_n', 'form': 'plain', 'ik_w': 10, 'ik_h': 20, 'inum_filt': 200, 'iker_sz': 11, 'iperm': 4, 'iinp_drop': 0.3, 'ifeat_drop': 0.2, 'ihid_drop': 0.3, 'att': True, 'head_num': 2}\n",
            "{'att': True,\n",
            " 'batch_size': 256,\n",
            " 'bias': True,\n",
            " 'config_dir': './config/',\n",
            " 'dataset': 'WN18RR',\n",
            " 'dropout': 0.4,\n",
            " 'embed_dim': 200,\n",
            " 'feat_drop': 0.3,\n",
            " 'form': 'plain',\n",
            " 'gamma': 40.0,\n",
            " 'gcn_dim': 200,\n",
            " 'gcn_layer': 1,\n",
            " 'gpu': '0',\n",
            " 'head_num': 2,\n",
            " 'hid_drop': 0.3,\n",
            " 'hid_drop2': 0.3,\n",
            " 'ifeat_drop': 0.2,\n",
            " 'ihid_drop': 0.3,\n",
            " 'iinp_drop': 0.3,\n",
            " 'ik_h': 20,\n",
            " 'ik_w': 10,\n",
            " 'iker_sz': 11,\n",
            " 'init_dim': 100,\n",
            " 'inum_filt': 200,\n",
            " 'iperm': 4,\n",
            " 'k_h': 20,\n",
            " 'k_w': 10,\n",
            " 'ker_sz': 7,\n",
            " 'l2': 0.0,\n",
            " 'lbl_smooth': 0.1,\n",
            " 'log_dir': '/content/drive/MyDrive/RAGAT/log/',\n",
            " 'lr': 0.001,\n",
            " 'max_epochs': 200,\n",
            " 'model': 'ragat',\n",
            " 'name': 'testrun_26_10_2022_08:27:57',\n",
            " 'neg_num': 1000,\n",
            " 'num_filt': 200,\n",
            " 'num_workers': 10,\n",
            " 'opn': 'cross',\n",
            " 'restore': False,\n",
            " 'score_func': 'interacte',\n",
            " 'seed': 41504,\n",
            " 'strategy': 'one_to_n',\n",
            " 'test_batch_size': 256}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-10-26 08:31:49,598 - [INFO] - [Evaluating Epoch 0 valid]: \n",
            "\tMRR: Tail : 0.00017, Head : 0.00025, Avg : 0.00021\n",
            "\n",
            "2022-10-26 08:31:52,359 - [INFO] - [Epoch 0]: Training Loss: 0.03817, Best valid MRR: 0.00021\n",
            "\n",
            "\n",
            "2022-10-26 08:35:42,723 - [INFO] - [Evaluating Epoch 1 valid]: \n",
            "\tMRR: Tail : 0.00053, Head : 0.00027, Avg : 0.0004\n",
            "\n",
            "2022-10-26 08:35:45,606 - [INFO] - [Epoch 1]: Training Loss: 0.0012887, Best valid MRR: 0.0004\n",
            "\n",
            "\n",
            "2022-10-26 08:39:35,030 - [INFO] - [Evaluating Epoch 2 valid]: \n",
            "\tMRR: Tail : 0.00889, Head : 0.00095, Avg : 0.00492\n",
            "\n",
            "2022-10-26 08:39:38,077 - [INFO] - [Epoch 2]: Training Loss: 0.00085795, Best valid MRR: 0.00492\n",
            "\n",
            "\n",
            "2022-10-26 08:43:27,045 - [INFO] - [Evaluating Epoch 3 valid]: \n",
            "\tMRR: Tail : 0.01413, Head : 0.00426, Avg : 0.0092\n",
            "\n",
            "2022-10-26 08:43:29,882 - [INFO] - [Epoch 3]: Training Loss: 0.0007477, Best valid MRR: 0.0092\n",
            "\n",
            "\n",
            "2022-10-26 08:47:18,130 - [INFO] - [Evaluating Epoch 4 valid]: \n",
            "\tMRR: Tail : 0.01827, Head : 0.00447, Avg : 0.01137\n",
            "\n",
            "2022-10-26 08:47:20,956 - [INFO] - [Epoch 4]: Training Loss: 0.00070823, Best valid MRR: 0.01137\n",
            "\n",
            "\n",
            "2022-10-26 08:51:09,772 - [INFO] - [Evaluating Epoch 5 valid]: \n",
            "\tMRR: Tail : 0.02376, Head : 0.00471, Avg : 0.01423\n",
            "\n",
            "2022-10-26 08:51:12,742 - [INFO] - [Epoch 5]: Training Loss: 0.00068968, Best valid MRR: 0.01423\n",
            "\n",
            "\n",
            "2022-10-26 08:55:01,272 - [INFO] - [Evaluating Epoch 6 valid]: \n",
            "\tMRR: Tail : 0.02637, Head : 0.01074, Avg : 0.01855\n",
            "\n",
            "2022-10-26 08:55:04,169 - [INFO] - [Epoch 6]: Training Loss: 0.00067966, Best valid MRR: 0.01855\n",
            "\n",
            "\n",
            "2022-10-26 08:58:52,359 - [INFO] - [Evaluating Epoch 7 valid]: \n",
            "\tMRR: Tail : 0.02625, Head : 0.01375, Avg : 0.02\n",
            "\n",
            "2022-10-26 08:58:55,185 - [INFO] - [Epoch 7]: Training Loss: 0.0006729, Best valid MRR: 0.02\n",
            "\n",
            "\n",
            "2022-10-26 09:02:43,620 - [INFO] - [Evaluating Epoch 8 valid]: \n",
            "\tMRR: Tail : 0.02618, Head : 0.01409, Avg : 0.02014\n",
            "\n",
            "2022-10-26 09:02:46,435 - [INFO] - [Epoch 8]: Training Loss: 0.0006686, Best valid MRR: 0.02014\n",
            "\n",
            "\n",
            "2022-10-26 09:06:35,168 - [INFO] - [Evaluating Epoch 9 valid]: \n",
            "\tMRR: Tail : 0.02811, Head : 0.01461, Avg : 0.02136\n",
            "\tMR: Tail : 9455.2, Head : 1.4863e+04, Avg : 1.2159e+04\n",
            "\tHit-1: Tail : 0.0145, Head : 0.00956, Avg : 0.01203\n",
            "\tHit-3: Tail : 0.02703, Head : 0.01549, Avg : 0.02126\n",
            "\tHit-10: Tail : 0.0557, Head : 0.02373, Avg : 0.03972\n",
            "2022-10-26 09:06:38,076 - [INFO] - [Epoch 9]: Training Loss: 0.000666, Best valid MRR: 0.02136\n",
            "\n",
            "\n",
            "2022-10-26 09:10:26,838 - [INFO] - [Evaluating Epoch 10 valid]: \n",
            "\tMRR: Tail : 0.02547, Head : 0.01444, Avg : 0.01996\n",
            "\n",
            "2022-10-26 09:10:26,842 - [INFO] - [Epoch 10]: Training Loss: 0.00066291, Best valid MRR: 0.02136\n",
            "\n",
            "\n",
            "2022-10-26 09:14:15,312 - [INFO] - [Evaluating Epoch 11 valid]: \n",
            "\tMRR: Tail : 0.02687, Head : 0.01495, Avg : 0.02091\n",
            "\n",
            "2022-10-26 09:14:15,314 - [INFO] - [Epoch 11]: Training Loss: 0.00066049, Best valid MRR: 0.02136\n",
            "\n",
            "\n",
            "2022-10-26 09:18:03,860 - [INFO] - [Evaluating Epoch 12 valid]: \n",
            "\tMRR: Tail : 0.02647, Head : 0.01486, Avg : 0.02066\n",
            "\n",
            "2022-10-26 09:18:03,867 - [INFO] - [Epoch 12]: Training Loss: 0.00065952, Best valid MRR: 0.02136\n",
            "\n",
            "\n",
            "2022-10-26 09:21:52,668 - [INFO] - [Evaluating Epoch 13 valid]: \n",
            "\tMRR: Tail : 0.02845, Head : 0.01469, Avg : 0.02157\n",
            "\n",
            "2022-10-26 09:21:55,476 - [INFO] - [Epoch 13]: Training Loss: 0.00065962, Best valid MRR: 0.02157\n",
            "\n",
            "\n",
            "2022-10-26 09:25:43,945 - [INFO] - [Evaluating Epoch 14 valid]: \n",
            "\tMRR: Tail : 0.02645, Head : 0.01511, Avg : 0.02078\n",
            "\n",
            "2022-10-26 09:25:43,948 - [INFO] - [Epoch 14]: Training Loss: 0.00065813, Best valid MRR: 0.02157\n",
            "\n",
            "\n",
            "2022-10-26 09:29:32,804 - [INFO] - [Evaluating Epoch 15 valid]: \n",
            "\tMRR: Tail : 0.0245, Head : 0.01529, Avg : 0.01989\n",
            "\n",
            "2022-10-26 09:29:32,808 - [INFO] - [Epoch 15]: Training Loss: 0.00065709, Best valid MRR: 0.02157\n",
            "\n",
            "\n",
            "2022-10-26 09:33:21,686 - [INFO] - [Evaluating Epoch 16 valid]: \n",
            "\tMRR: Tail : 0.02585, Head : 0.01393, Avg : 0.01989\n",
            "\n",
            "2022-10-26 09:33:21,688 - [INFO] - [Epoch 16]: Training Loss: 0.00065575, Best valid MRR: 0.02157\n",
            "\n",
            "\n",
            "2022-10-26 09:37:09,981 - [INFO] - [Evaluating Epoch 17 valid]: \n",
            "\tMRR: Tail : 0.02288, Head : 0.01565, Avg : 0.01927\n",
            "\n",
            "2022-10-26 09:37:09,983 - [INFO] - [Epoch 17]: Training Loss: 0.00065287, Best valid MRR: 0.02157\n",
            "\n",
            "\n",
            "2022-10-26 09:40:58,539 - [INFO] - [Evaluating Epoch 18 valid]: \n",
            "\tMRR: Tail : 0.03079, Head : 0.0156, Avg : 0.0232\n",
            "\n",
            "2022-10-26 09:41:01,350 - [INFO] - [Epoch 18]: Training Loss: 0.0006499, Best valid MRR: 0.0232\n",
            "\n",
            "\n",
            "2022-10-26 09:44:50,066 - [INFO] - [Evaluating Epoch 19 valid]: \n",
            "\tMRR: Tail : 0.02517, Head : 0.01606, Avg : 0.02062\n",
            "\tMR: Tail : 8515.8, Head : 1.2593e+04, Avg : 1.0555e+04\n",
            "\tHit-1: Tail : 0.01483, Head : 0.00989, Avg : 0.01236\n",
            "\tHit-3: Tail : 0.0267, Head : 0.01615, Avg : 0.02142\n",
            "\tHit-10: Tail : 0.04153, Head : 0.02637, Avg : 0.03395\n",
            "2022-10-26 09:44:50,076 - [INFO] - [Epoch 19]: Training Loss: 0.00064681, Best valid MRR: 0.0232\n",
            "\n",
            "\n",
            "2022-10-26 09:48:38,613 - [INFO] - [Evaluating Epoch 20 valid]: \n",
            "\tMRR: Tail : 0.02281, Head : 0.01531, Avg : 0.01906\n",
            "\n",
            "2022-10-26 09:48:38,617 - [INFO] - [Epoch 20]: Training Loss: 0.00064375, Best valid MRR: 0.0232\n",
            "\n",
            "\n",
            "2022-10-26 09:52:27,343 - [INFO] - [Evaluating Epoch 21 valid]: \n",
            "\tMRR: Tail : 0.02721, Head : 0.01703, Avg : 0.02212\n",
            "\n",
            "2022-10-26 09:52:27,346 - [INFO] - [Epoch 21]: Training Loss: 0.00063994, Best valid MRR: 0.0232\n",
            "\n",
            "\n",
            "2022-10-26 09:56:15,989 - [INFO] - [Evaluating Epoch 22 valid]: \n",
            "\tMRR: Tail : 0.03557, Head : 0.01683, Avg : 0.0262\n",
            "\n",
            "2022-10-26 09:56:19,003 - [INFO] - [Epoch 22]: Training Loss: 0.00063774, Best valid MRR: 0.0262\n",
            "\n",
            "\n",
            "2022-10-26 10:00:07,746 - [INFO] - [Evaluating Epoch 23 valid]: \n",
            "\tMRR: Tail : 0.03121, Head : 0.01697, Avg : 0.02409\n",
            "\n",
            "2022-10-26 10:00:07,749 - [INFO] - [Epoch 23]: Training Loss: 0.00063578, Best valid MRR: 0.0262\n",
            "\n",
            "\n",
            "2022-10-26 10:03:56,650 - [INFO] - [Evaluating Epoch 24 valid]: \n",
            "\tMRR: Tail : 0.02243, Head : 0.01655, Avg : 0.01949\n",
            "\n",
            "2022-10-26 10:03:56,653 - [INFO] - [Epoch 24]: Training Loss: 0.00063257, Best valid MRR: 0.0262\n",
            "\n",
            "\n",
            "2022-10-26 10:07:45,176 - [INFO] - [Evaluating Epoch 25 valid]: \n",
            "\tMRR: Tail : 0.03114, Head : 0.01724, Avg : 0.02419\n",
            "\n",
            "2022-10-26 10:07:45,179 - [INFO] - [Epoch 25]: Training Loss: 0.0006299, Best valid MRR: 0.0262\n",
            "\n",
            "\n",
            "2022-10-26 10:11:33,707 - [INFO] - [Evaluating Epoch 26 valid]: \n",
            "\tMRR: Tail : 0.03461, Head : 0.0169, Avg : 0.02575\n",
            "\n",
            "2022-10-26 10:11:33,715 - [INFO] - [Epoch 26]: Training Loss: 0.0006259, Best valid MRR: 0.0262\n",
            "\n",
            "\n",
            "2022-10-26 10:15:22,294 - [INFO] - [Evaluating Epoch 27 valid]: \n",
            "\tMRR: Tail : 0.03224, Head : 0.01899, Avg : 0.02561\n",
            "\n",
            "2022-10-26 10:15:22,298 - [INFO] - [Epoch 27]: Training Loss: 0.00062275, Best valid MRR: 0.0262\n",
            "\n",
            "\n",
            "2022-10-26 10:19:10,743 - [INFO] - [Evaluating Epoch 28 valid]: \n",
            "\tMRR: Tail : 0.0382, Head : 0.01955, Avg : 0.02887\n",
            "\n",
            "2022-10-26 10:19:13,650 - [INFO] - [Epoch 28]: Training Loss: 0.00061968, Best valid MRR: 0.02887\n",
            "\n",
            "\n",
            "2022-10-26 10:23:02,344 - [INFO] - [Evaluating Epoch 29 valid]: \n",
            "\tMRR: Tail : 0.04197, Head : 0.02151, Avg : 0.03174\n",
            "\tMR: Tail : 4255.0, Head : 7136.8, Avg : 5695.9\n",
            "\tHit-1: Tail : 0.02109, Head : 0.01252, Avg : 0.01681\n",
            "\tHit-3: Tail : 0.04483, Head : 0.02011, Avg : 0.03247\n",
            "\tHit-10: Tail : 0.08273, Head : 0.03494, Avg : 0.05883\n",
            "2022-10-26 10:23:05,152 - [INFO] - [Epoch 29]: Training Loss: 0.00061438, Best valid MRR: 0.03174\n",
            "\n",
            "\n",
            "2022-10-26 10:26:53,981 - [INFO] - [Evaluating Epoch 30 valid]: \n",
            "\tMRR: Tail : 0.04734, Head : 0.02479, Avg : 0.03606\n",
            "\n",
            "2022-10-26 10:26:56,912 - [INFO] - [Epoch 30]: Training Loss: 0.00060813, Best valid MRR: 0.03606\n",
            "\n",
            "\n",
            "2022-10-26 10:30:45,737 - [INFO] - [Evaluating Epoch 31 valid]: \n",
            "\tMRR: Tail : 0.05337, Head : 0.02725, Avg : 0.04031\n",
            "\n",
            "2022-10-26 10:30:48,656 - [INFO] - [Epoch 31]: Training Loss: 0.00060371, Best valid MRR: 0.04031\n",
            "\n",
            "\n",
            "2022-10-26 10:34:37,338 - [INFO] - [Evaluating Epoch 32 valid]: \n",
            "\tMRR: Tail : 0.06277, Head : 0.03177, Avg : 0.04727\n",
            "\n",
            "2022-10-26 10:34:40,351 - [INFO] - [Epoch 32]: Training Loss: 0.00059952, Best valid MRR: 0.04727\n",
            "\n",
            "\n",
            "2022-10-26 10:38:29,268 - [INFO] - [Evaluating Epoch 33 valid]: \n",
            "\tMRR: Tail : 0.07535, Head : 0.04848, Avg : 0.06191\n",
            "\n",
            "2022-10-26 10:38:32,222 - [INFO] - [Epoch 33]: Training Loss: 0.00059326, Best valid MRR: 0.06191\n",
            "\n",
            "\n",
            "2022-10-26 10:42:21,353 - [INFO] - [Evaluating Epoch 34 valid]: \n",
            "\tMRR: Tail : 0.0634, Head : 0.03887, Avg : 0.05114\n",
            "\n",
            "2022-10-26 10:42:21,358 - [INFO] - [Epoch 34]: Training Loss: 0.00058733, Best valid MRR: 0.06191\n",
            "\n",
            "\n",
            "2022-10-26 10:46:09,682 - [INFO] - [Evaluating Epoch 35 valid]: \n",
            "\tMRR: Tail : 0.06445, Head : 0.03929, Avg : 0.05187\n",
            "\n",
            "2022-10-26 10:46:09,688 - [INFO] - [Epoch 35]: Training Loss: 0.00058083, Best valid MRR: 0.06191\n",
            "\n",
            "\n",
            "2022-10-26 10:49:58,438 - [INFO] - [Evaluating Epoch 36 valid]: \n",
            "\tMRR: Tail : 0.07517, Head : 0.04954, Avg : 0.06235\n",
            "\n",
            "2022-10-26 10:50:01,209 - [INFO] - [Epoch 36]: Training Loss: 0.00057401, Best valid MRR: 0.06235\n",
            "\n",
            "\n",
            "2022-10-26 10:53:49,648 - [INFO] - [Evaluating Epoch 37 valid]: \n",
            "\tMRR: Tail : 0.1146, Head : 0.07841, Avg : 0.09651\n",
            "\n",
            "2022-10-26 10:53:52,723 - [INFO] - [Epoch 37]: Training Loss: 0.00056744, Best valid MRR: 0.09651\n",
            "\n",
            "\n",
            "2022-10-26 10:57:41,495 - [INFO] - [Evaluating Epoch 38 valid]: \n",
            "\tMRR: Tail : 0.11791, Head : 0.0834, Avg : 0.10065\n",
            "\n",
            "2022-10-26 10:57:44,344 - [INFO] - [Epoch 38]: Training Loss: 0.00056019, Best valid MRR: 0.10065\n",
            "\n",
            "\n",
            "2022-10-26 11:01:33,155 - [INFO] - [Evaluating Epoch 39 valid]: \n",
            "\tMRR: Tail : 0.10241, Head : 0.07736, Avg : 0.08988\n",
            "\tMR: Tail : 3423.6, Head : 4823.7, Avg : 4123.6\n",
            "\tHit-1: Tail : 0.05834, Head : 0.04252, Avg : 0.05043\n",
            "\tHit-3: Tail : 0.11305, Head : 0.08471, Avg : 0.09888\n",
            "\tHit-10: Tail : 0.18392, Head : 0.14305, Avg : 0.16348\n",
            "2022-10-26 11:01:33,158 - [INFO] - [Epoch 39]: Training Loss: 0.00055418, Best valid MRR: 0.10065\n",
            "\n",
            "\n",
            "2022-10-26 11:05:21,679 - [INFO] - [Evaluating Epoch 40 valid]: \n",
            "\tMRR: Tail : 0.16807, Head : 0.1288, Avg : 0.14844\n",
            "\n",
            "2022-10-26 11:05:24,760 - [INFO] - [Epoch 40]: Training Loss: 0.00054738, Best valid MRR: 0.14844\n",
            "\n",
            "\n",
            "2022-10-26 11:09:13,514 - [INFO] - [Evaluating Epoch 41 valid]: \n",
            "\tMRR: Tail : 0.15337, Head : 0.12583, Avg : 0.1396\n",
            "\n",
            "2022-10-26 11:09:13,521 - [INFO] - [Epoch 41]: Training Loss: 0.00053959, Best valid MRR: 0.14844\n",
            "\n",
            "\n",
            "2022-10-26 11:13:01,554 - [INFO] - [Evaluating Epoch 42 valid]: \n",
            "\tMRR: Tail : 0.20361, Head : 0.16668, Avg : 0.18514\n",
            "\n",
            "2022-10-26 11:13:04,504 - [INFO] - [Epoch 42]: Training Loss: 0.00053503, Best valid MRR: 0.18514\n",
            "\n",
            "\n",
            "2022-10-26 11:16:53,025 - [INFO] - [Evaluating Epoch 43 valid]: \n",
            "\tMRR: Tail : 0.22906, Head : 0.18876, Avg : 0.20891\n",
            "\n",
            "2022-10-26 11:16:56,019 - [INFO] - [Epoch 43]: Training Loss: 0.00052873, Best valid MRR: 0.20891\n",
            "\n",
            "\n",
            "2022-10-26 11:20:44,982 - [INFO] - [Evaluating Epoch 44 valid]: \n",
            "\tMRR: Tail : 0.21699, Head : 0.17518, Avg : 0.19609\n",
            "\n",
            "2022-10-26 11:20:44,984 - [INFO] - [Epoch 44]: Training Loss: 0.00052294, Best valid MRR: 0.20891\n",
            "\n",
            "\n",
            "2022-10-26 11:24:32,994 - [INFO] - [Evaluating Epoch 45 valid]: \n",
            "\tMRR: Tail : 0.22544, Head : 0.20358, Avg : 0.21451\n",
            "\n",
            "2022-10-26 11:24:35,812 - [INFO] - [Epoch 45]: Training Loss: 0.00051941, Best valid MRR: 0.21451\n",
            "\n",
            "\n",
            "2022-10-26 11:28:24,825 - [INFO] - [Evaluating Epoch 46 valid]: \n",
            "\tMRR: Tail : 0.27593, Head : 0.23874, Avg : 0.25733\n",
            "\n",
            "2022-10-26 11:28:27,778 - [INFO] - [Epoch 46]: Training Loss: 0.0005152, Best valid MRR: 0.25733\n",
            "\n",
            "\n",
            "2022-10-26 11:32:16,581 - [INFO] - [Evaluating Epoch 47 valid]: \n",
            "\tMRR: Tail : 0.24728, Head : 0.21694, Avg : 0.23211\n",
            "\n",
            "2022-10-26 11:32:16,589 - [INFO] - [Epoch 47]: Training Loss: 0.00050957, Best valid MRR: 0.25733\n",
            "\n",
            "\n",
            "2022-10-26 11:36:04,562 - [INFO] - [Evaluating Epoch 48 valid]: \n",
            "\tMRR: Tail : 0.30059, Head : 0.26887, Avg : 0.28473\n",
            "\n",
            "2022-10-26 11:36:07,542 - [INFO] - [Epoch 48]: Training Loss: 0.0005062, Best valid MRR: 0.28473\n",
            "\n",
            "\n",
            "2022-10-26 11:39:56,967 - [INFO] - [Evaluating Epoch 49 valid]: \n",
            "\tMRR: Tail : 0.30522, Head : 0.27529, Avg : 0.29025\n",
            "\tMR: Tail : 2404.1, Head : 3692.0, Avg : 3048.1\n",
            "\tHit-1: Tail : 0.22874, Head : 0.21259, Avg : 0.22067\n",
            "\tHit-3: Tail : 0.34311, Head : 0.30653, Avg : 0.32482\n",
            "\tHit-10: Tail : 0.45188, Head : 0.39189, Avg : 0.42189\n",
            "2022-10-26 11:39:59,912 - [INFO] - [Epoch 49]: Training Loss: 0.00050161, Best valid MRR: 0.29025\n",
            "\n",
            "\n",
            "2022-10-26 11:43:48,413 - [INFO] - [Evaluating Epoch 50 valid]: \n",
            "\tMRR: Tail : 0.30423, Head : 0.26056, Avg : 0.28239\n",
            "\n",
            "2022-10-26 11:43:48,415 - [INFO] - [Epoch 50]: Training Loss: 0.00049845, Best valid MRR: 0.29025\n",
            "\n",
            "\n",
            "2022-10-26 11:47:37,008 - [INFO] - [Evaluating Epoch 51 valid]: \n",
            "\tMRR: Tail : 0.32709, Head : 0.28303, Avg : 0.30506\n",
            "\n",
            "2022-10-26 11:47:39,865 - [INFO] - [Epoch 51]: Training Loss: 0.00049686, Best valid MRR: 0.30506\n",
            "\n",
            "\n",
            "2022-10-26 11:51:28,231 - [INFO] - [Evaluating Epoch 52 valid]: \n",
            "\tMRR: Tail : 0.34838, Head : 0.30335, Avg : 0.32586\n",
            "\n",
            "2022-10-26 11:51:31,242 - [INFO] - [Epoch 52]: Training Loss: 0.00049288, Best valid MRR: 0.32586\n",
            "\n",
            "\n",
            "2022-10-26 11:55:19,787 - [INFO] - [Evaluating Epoch 53 valid]: \n",
            "\tMRR: Tail : 0.34102, Head : 0.29782, Avg : 0.31942\n",
            "\n",
            "2022-10-26 11:55:19,793 - [INFO] - [Epoch 53]: Training Loss: 0.00049075, Best valid MRR: 0.32586\n",
            "\n",
            "\n",
            "2022-10-26 11:59:07,548 - [INFO] - [Evaluating Epoch 54 valid]: \n",
            "\tMRR: Tail : 0.36092, Head : 0.30852, Avg : 0.33472\n",
            "\n",
            "2022-10-26 11:59:10,501 - [INFO] - [Epoch 54]: Training Loss: 0.00048892, Best valid MRR: 0.33472\n",
            "\n",
            "\n",
            "2022-10-26 12:02:58,764 - [INFO] - [Evaluating Epoch 55 valid]: \n",
            "\tMRR: Tail : 0.37811, Head : 0.33092, Avg : 0.35452\n",
            "\n",
            "2022-10-26 12:03:01,782 - [INFO] - [Epoch 55]: Training Loss: 0.00048517, Best valid MRR: 0.35452\n",
            "\n",
            "\n",
            "2022-10-26 12:06:50,430 - [INFO] - [Evaluating Epoch 56 valid]: \n",
            "\tMRR: Tail : 0.37162, Head : 0.32533, Avg : 0.34847\n",
            "\n",
            "2022-10-26 12:06:50,433 - [INFO] - [Epoch 56]: Training Loss: 0.00048265, Best valid MRR: 0.35452\n",
            "\n",
            "\n",
            "2022-10-26 12:10:38,619 - [INFO] - [Evaluating Epoch 57 valid]: \n",
            "\tMRR: Tail : 0.38992, Head : 0.33721, Avg : 0.36357\n",
            "\n",
            "2022-10-26 12:10:41,576 - [INFO] - [Epoch 57]: Training Loss: 0.00048045, Best valid MRR: 0.36357\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = Runner(args)\n",
        "model.fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dB8DItHV2bfd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "44e7e1b8fa2096bd5707ed7fd18b1724a2db25f4c565a7673f8b6e7bfc49d25d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
